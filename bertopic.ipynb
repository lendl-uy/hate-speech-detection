{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from dataset.dataset import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from bertopic import BERTopic\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from constants import CLEANED_DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from dataset/cleaned_dataset.pkl\n",
      "Shape X_train (8674,)\n",
      "Shape X_test (3976,)\n",
      "Shape X_val (3859,)\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(full_data_path=CLEANED_DATASET_PATH,\n",
    "                  from_scratch=False,\n",
    "                  split_sizes=[8674, 3859, 3976])\n",
    "dataset.build()\n",
    "\n",
    "X_train = dataset.get_features(split_type=\"train\")\n",
    "Y_train = dataset.get_labels(split_type=\"train\")\n",
    "X_val = dataset.get_features(split_type=\"val\")\n",
    "Y_val = dataset.get_labels(split_type=\"val\")\n",
    "X_test = dataset.get_features(split_type=\"test\")\n",
    "Y_test = dataset.get_labels(split_type=\"test\")\n",
    "\n",
    "print(\"Shape X_train\",np.shape(X_train))\n",
    "print(\"Shape X_test\",np.shape(X_test))\n",
    "print(\"Shape X_val\",np.shape(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16509\n",
      "16509\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(np.concatenate((X_train,X_test,X_val)))))\n",
    "print(len(np.unique(X_val)) + len(np.unique(X_test)) + len(np.unique(X_train)))\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "s_bert = BERTopic(calculate_probabilities=True) #true for soft clustering multiple topics per documents, false for one topic per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model on X\n",
    "topics, probs = s_bert.fit_transform(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  8674\n",
      "Number of Features(topics):  106\n"
     ]
    }
   ],
   "source": [
    "datapoints, features = np.shape(probs)\n",
    "print(\"Number of rows: \", datapoints)\n",
    "print(\"Number of Features(topics): \", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_bert.get_topic_info() #Hard clustering name = topics, representation = keywords, representative docs = docs with that topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_bert.get_document_info(X_train) #Info on each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_t, probs_t = s_bert.transform(X_test) #Fitting test set into different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#^Sparse efficiency warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  3976\n",
      "Number of Features(topics):  106\n"
     ]
    }
   ],
   "source": [
    "datapoints_t, features_t = np.shape(probs_t) #should have same features with train set\n",
    "print(\"Number of rows: \", datapoints_t)\n",
    "print(\"Number of Features(topics): \", features_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bert-RF Model Accuracy: 0.5995975855130785\n",
      "Best Bert-RF Model F1-Score: 0.5743315508021389\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=511, random_state=0))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_pipeline.fit(probs, Y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "Y_pred = rf_pipeline.predict(probs_t)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(f\"Best Bert-RF Model Accuracy: {accuracy}\")\n",
    "print(f\"Best Bert-RF Model F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Multilanguage Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "sm_bert = BERTopic(calculate_probabilities=True, language = \"multilingual\") #true for soft clustering multiple topics per documents, false for one topic per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting X_train into Bert\n",
      "X_train Shape:  (8674, 115)\n",
      "Using model to transform X_test\n",
      "X_test Shape:  (3976, 115)\n",
      "Best Bert-RF Model Accuracy: 0.5995975855130785\n",
      "Best Bert-RF Model F1-Score: 0.5708894878706199\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting X_train into Bert\")\n",
    "topics, probs = sm_bert.fit_transform(X_train)\n",
    "print(\"X_train Shape: \", np.shape(probs))\n",
    "\n",
    "print(\"Using model to transform X_test\")\n",
    "topics_t, probs_t = sm_bert.transform(X_test) #Fitting test set into different topics\n",
    "print(\"X_test Shape: \", np.shape(probs_t))\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_pipeline.fit(probs, Y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "Y_pred = rf_pipeline.predict(probs_t)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(f\"Best Bert-RF Model Accuracy: {accuracy}\")\n",
    "print(f\"Best Bert-RF Model F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bert-RF Model Accuracy: 0.5995975855130785\n",
      "Best Bert-RF Model F1-Score: 0.5708894878706199\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline\n",
    "rf_pipeline.fit(probs, Y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "Y_pred = rf_pipeline.predict(probs_t)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(f\"Best Bert-RF Model Accuracy: {accuracy}\")\n",
    "print(f\"Best Bert-RF Model F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Bert Reduced Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "sm_bert = BERTopic(calculate_probabilities=True, language = \"multilingual\") #true for soft clustering multiple topics per documents, false for one topic per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting X_train into Bert\n",
      "X_train Shape:  (8674, 128)\n",
      "Using model to transform X_test\n",
      "X_test Shape:  (3976, 128)\n",
      "Best Bert-RF Model Accuracy: 0.5870221327967807\n",
      "Best Bert-RF Model F1-Score: 0.5484048404840484\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting X_train into Bert\")\n",
    "topics, probs = sm_bert.fit_transform(X_train)\n",
    "print(\"X_train Shape: \", np.shape(probs))\n",
    "\n",
    "print(\"Using model to transform X_test\")\n",
    "topics_t, probs_t = sm_bert.transform(X_test) #Fitting test set into different topics\n",
    "print(\"X_test Shape: \", np.shape(probs_t))\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_pipeline.fit(probs, Y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "Y_pred = rf_pipeline.predict(probs_t)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(f\"Best Bert-RF Model Accuracy: {accuracy}\")\n",
    "print(f\"Best Bert-RF Model F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your own Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representations with \n",
    "# a `bertopic.representation` model\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic represenations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
