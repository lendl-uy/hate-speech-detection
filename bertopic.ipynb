{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from dataset.dataset import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from bertopic import BERTopic\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from constants import CLEANED_DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from dataset/cleaned_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(full_data_path=CLEANED_DATASET_PATH,\n",
    "                  from_scratch=False,\n",
    "                  split_sizes=[10000, 4232, 4232])\n",
    "dataset.build()\n",
    "\n",
    "X_train = dataset.get_features(split_type=\"train\")\n",
    "Y_train = dataset.get_labels(split_type=\"train\")\n",
    "X_val = dataset.get_features(split_type=\"val\")\n",
    "Y_val = dataset.get_labels(split_type=\"val\")\n",
    "X_test = dataset.get_features(split_type=\"test\")\n",
    "Y_test = dataset.get_labels(split_type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "s_bert = BERTopic(calculate_probabilities=True) #true for soft clustering multiple topics per documents, false for one topic per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model on X\n",
    "topics, probs = s_bert.fit_transform(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  10000\n",
      "Number of Features(topics):  158\n"
     ]
    }
   ],
   "source": [
    "datapoints, features = np.shape(probs)\n",
    "print(\"Number of rows: \", datapoints)\n",
    "print(\"Number of Features(topics): \", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>3579</td>\n",
       "      <td>-1_binay_poe_mo_si</td>\n",
       "      <td>[binay, poe, mo, si, duterte, kay, mar, roxas,...</td>\n",
       "      <td>[poe duterte go grace poe, rt kay binay, si bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>817</td>\n",
       "      <td>0_mar_roxas_si_pag</td>\n",
       "      <td>[mar, roxas, si, pag, matuwid, nanalo, daang, ...</td>\n",
       "      <td>[wag lang si mar roxas, wag lang si mar roxas,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>1_vp_vice_mayor_presidente</td>\n",
       "      <td>[vp, vice, mayor, presidente, president, jejom...</td>\n",
       "      <td>[puro makati pinagmamalaki binay nga yung naga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>2_poe_grace_si_manalo</td>\n",
       "      <td>[poe, grace, si, manalo, duterte, lang, sen, w...</td>\n",
       "      <td>[sec mar mag give way kay sen grace kasi mas c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>162</td>\n",
       "      <td>3_vote_mar_presidential_need</td>\n",
       "      <td>[vote, mar, presidential, need, election, win,...</td>\n",
       "      <td>[mar roxas need ofw vote, mar roxas need ofw v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>153</td>\n",
       "      <td>11</td>\n",
       "      <td>153_dutertetilltheend_uneducated_ider_passbooks</td>\n",
       "      <td>[dutertetilltheend, uneducated, ider, passbook...</td>\n",
       "      <td>[mar roxas dutertetilltheend, mar roxas uneduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>154</td>\n",
       "      <td>11</td>\n",
       "      <td>154_tvc_fan_picture_yupangco</td>\n",
       "      <td>[tvc, fan, picture, yupangco, masabing, ehe, d...</td>\n",
       "      <td>[kairita yung tvc binay nang damay ibang tao e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>155</td>\n",
       "      <td>11</td>\n",
       "      <td>155_duterte2016_paid_resolve_columns</td>\n",
       "      <td>[duterte2016, paid, resolve, columns, postives...</td>\n",
       "      <td>[duterte2016, duterte2016, duterte2016]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>156</td>\n",
       "      <td>10</td>\n",
       "      <td>156_political_ad_260m_nabwiset</td>\n",
       "      <td>[political, ad, 260m, nabwiset, galang, 300m, ...</td>\n",
       "      <td>[ba talaga ititigil ung political ad binay maw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>157</td>\n",
       "      <td>10</td>\n",
       "      <td>157_grace_norzagaray_laughingstock_katapusan</td>\n",
       "      <td>[grace, norzagaray, laughingstock, katapusan, ...</td>\n",
       "      <td>[naman also know digong grace poe making laugh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                             Name  \\\n",
       "0       -1   3579                               -1_binay_poe_mo_si   \n",
       "1        0    817                               0_mar_roxas_si_pag   \n",
       "2        1    337                       1_vp_vice_mayor_presidente   \n",
       "3        2    242                            2_poe_grace_si_manalo   \n",
       "4        3    162                     3_vote_mar_presidential_need   \n",
       "..     ...    ...                                              ...   \n",
       "154    153     11  153_dutertetilltheend_uneducated_ider_passbooks   \n",
       "155    154     11                     154_tvc_fan_picture_yupangco   \n",
       "156    155     11             155_duterte2016_paid_resolve_columns   \n",
       "157    156     10                   156_political_ad_260m_nabwiset   \n",
       "158    157     10     157_grace_norzagaray_laughingstock_katapusan   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [binay, poe, mo, si, duterte, kay, mar, roxas,...   \n",
       "1    [mar, roxas, si, pag, matuwid, nanalo, daang, ...   \n",
       "2    [vp, vice, mayor, presidente, president, jejom...   \n",
       "3    [poe, grace, si, manalo, duterte, lang, sen, w...   \n",
       "4    [vote, mar, presidential, need, election, win,...   \n",
       "..                                                 ...   \n",
       "154  [dutertetilltheend, uneducated, ider, passbook...   \n",
       "155  [tvc, fan, picture, yupangco, masabing, ehe, d...   \n",
       "156  [duterte2016, paid, resolve, columns, postives...   \n",
       "157  [political, ad, 260m, nabwiset, galang, 300m, ...   \n",
       "158  [grace, norzagaray, laughingstock, katapusan, ...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [poe duterte go grace poe, rt kay binay, si bi...  \n",
       "1    [wag lang si mar roxas, wag lang si mar roxas,...  \n",
       "2    [puro makati pinagmamalaki binay nga yung naga...  \n",
       "3    [sec mar mag give way kay sen grace kasi mas c...  \n",
       "4    [mar roxas need ofw vote, mar roxas need ofw v...  \n",
       "..                                                 ...  \n",
       "154  [mar roxas dutertetilltheend, mar roxas uneduc...  \n",
       "155  [kairita yung tvc binay nang damay ibang tao e...  \n",
       "156            [duterte2016, duterte2016, duterte2016]  \n",
       "157  [ba talaga ititigil ung political ad binay maw...  \n",
       "158  [naman also know digong grace poe making laugh...  \n",
       "\n",
       "[159 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_bert.get_topic_info() #Hard clustering name = topics, representation = keywords, representative docs = docs with that topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_bert.get_document_info(X_train) #Info on each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda\\Lib\\site-packages\\scipy\\sparse\\_index.py:146: SparseEfficiencyWarning:\n",
      "\n",
      "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_t, probs_t = s_bert.transform(X_test) #Fitting test set into different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#^Sparse efficiency warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  4232\n",
      "Number of Features(topics):  158\n"
     ]
    }
   ],
   "source": [
    "datapoints_t, features_t = np.shape(probs_t) #should have same features with train set\n",
    "print(\"Number of rows: \", datapoints_t)\n",
    "print(\"Number of Features(topics): \", features_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bert-RF Model Accuracy: 0.6068052930056711\n",
      "Best Bert-RF Model F1-Score: 0.5377777777777777\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=511, random_state=0))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_pipeline.fit(probs, Y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "Y_pred = rf_pipeline.predict(probs_t)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(f\"Best Bert-RF Model Accuracy: {accuracy}\")\n",
    "print(f\"Best Bert-RF Model F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Multilanguage Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "sm_bert = BERTopic(calculate_probabilities=True, language = \"multilingual\") #true for soft clustering multiple topics per documents, false for one topic per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting X_train into Bert\n",
      "X_train Shape:  (10000, 172)\n",
      "Using model to transform X_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\anaconda\\Lib\\site-packages\\scipy\\sparse\\_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test Shape:  (4232, 172)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rf_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test Shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mshape(probs_t))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Fit the pipeline\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m rf_pipeline\u001b[38;5;241m.\u001b[39mfit(probs, Y_train)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluate the pipeline\u001b[39;00m\n\u001b[0;32m     13\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m rf_pipeline\u001b[38;5;241m.\u001b[39mpredict(probs_t)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Fitting X_train into Bert\")\n",
    "topics, probs = sm_bert.fit_transform(X_train)\n",
    "print(\"X_train Shape: \", np.shape(probs))\n",
    "\n",
    "print(\"Using model to transform X_test\")\n",
    "topics_t, probs_t = sm_bert.transform(X_test) #Fitting test set into different topics\n",
    "print(\"X_test Shape: \", np.shape(probs_t))\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_pipeline.fit(probs, Y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "Y_pred = rf_pipeline.predict(probs_t)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(f\"Best Bert-RF Model Accuracy: {accuracy}\")\n",
    "print(f\"Best Bert-RF Model F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bert-RF Model Accuracy: 0.6198015122873346\n",
      "Best Bert-RF Model F1-Score: 0.5984527077614175\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline\n",
    "rf_pipeline.fit(probs, Y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "Y_pred = rf_pipeline.predict(probs_t)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(f\"Best Bert-RF Model Accuracy: {accuracy}\")\n",
    "print(f\"Best Bert-RF Model F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Bert Reduced Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "sm_bert = BERTopic(calculate_probabilities=True, language = \"multilingual\") #true for soft clustering multiple topics per documents, false for one topic per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting X_train into Bert\")\n",
    "topics, probs = sm_bert.fit_transform(X_train)\n",
    "print(\"X_train Shape: \", np.shape(probs))\n",
    "\n",
    "print(\"Using model to transform X_test\")\n",
    "topics_t, probs_t = sm_bert.transform(X_test) #Fitting test set into different topics\n",
    "print(\"X_test Shape: \", np.shape(probs_t))\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_pipeline.fit(probs, Y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "Y_pred = rf_pipeline.predict(probs_t)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(f\"Best Bert-RF Model Accuracy: {accuracy}\")\n",
    "print(f\"Best Bert-RF Model F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your own Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representations with \n",
    "# a `bertopic.representation` model\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic represenations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
