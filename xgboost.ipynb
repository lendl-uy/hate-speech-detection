{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c48592",
   "metadata": {},
   "source": [
    "# XGBoost Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778cef84980d4467",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:15:34.266840Z",
     "start_time": "2024-04-30T06:15:34.253920Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import optuna\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from dataset.dataset import Dataset\n",
    "from constants import CLEANED_DATASET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ffafa63beef650",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e92953c8b90f034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:15:35.143992Z",
     "start_time": "2024-04-30T06:15:35.093759Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d53039",
   "metadata": {},
   "source": [
    "## Load the 2016 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c26f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(full_data_path='dataset/cleaned_dataset_v1.pkl',\n",
    "                  from_scratch=False,\n",
    "                  split_sizes=[21282,2644,2492])\n",
    "dataset.build()\n",
    "\n",
    "X_train = dataset.get_features(split_type=\"train\")\n",
    "Y_train = dataset.get_labels(split_type=\"train\")\n",
    "X_val = dataset.get_features(split_type=\"val\")\n",
    "Y_val = dataset.get_labels(split_type=\"val\")\n",
    "X_test = dataset.get_features(split_type=\"test\")\n",
    "Y_test = dataset.get_labels(split_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_val_vectorized = vectorizer.transform(X_val)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920abdcf",
   "metadata": {},
   "source": [
    "## Train the XGBoost model on the 2016 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def create_objective(X_train, Y_train, X_test, Y_test):\n",
    "    def objective(trial):\n",
    "        # Suggest values for the hyperparameters\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "\n",
    "        # Create an XGBoost classifier model with suggested parameters\n",
    "        model = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            n_estimators=n_estimators,    # Number of trees\n",
    "            learning_rate=learning_rate,  # Learning rate\n",
    "            max_depth=max_depth,          # Depth of the trees\n",
    "            subsample=subsample,          # Subsampling of the training instances\n",
    "            colsample_bytree=colsample_bytree,  # Subsampling of columns for each tree\n",
    "            seed=RANDOM_SEED,             # Seed for reproducibility\n",
    "            use_label_encoder=False,      # Disable label encoder warning\n",
    "            eval_metric=\"logloss\")\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        # Predict the labels on the test set\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        accuracy = accuracy_score(Y_test, Y_pred)\n",
    "        return accuracy\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89379e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study object\n",
    "# study = optuna.create_study(direction=\"maximize\", study_name=f\"LDA_XGBoost_Pipeline\")\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=f\"XGBoost_Pipeline\")\n",
    "\n",
    "\n",
    "# Create the study objective\n",
    "objective = create_objective(X_train_vectorized, Y_train, X_val_vectorized, Y_val)\n",
    "\n",
    "# Execute an optimization\n",
    "study.optimize(objective, n_trials=20, n_jobs=-1)\n",
    "\n",
    "# Print the best trial results\n",
    "print(f\"Best Accuracy: {study.best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72739f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_estimators = study.best_trial.params[\"n_estimators\"]\n",
    "best_learning_rate = study.best_trial.params[\"learning_rate\"]\n",
    "best_max_depth = study.best_trial.params[\"max_depth\"]\n",
    "best_subsample = study.best_trial.params[\"subsample\"]\n",
    "best_colsample_bytree = study.best_trial.params[\"colsample_bytree\"]\n",
    "\n",
    "model = xgb.XGBClassifier(n_estimators=best_n_estimators, \n",
    "                                 learning_rate=best_learning_rate,\n",
    "                                 max_depth=best_max_depth,\n",
    "                                 random_state=RANDOM_SEED)\n",
    "model.fit(X_train_vectorized, Y_train)\n",
    "Y_pred = model.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "precision = precision_score(Y_test, Y_pred)\n",
    "recall = recall_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Best XGBoost Model Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be051016d14124",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load the 2016+2022 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88b506f040054ff5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:15:47.281786Z",
     "start_time": "2024-04-30T06:15:35.782572Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from dataset/cleaned_2022dataset_v1.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(full_data_path='dataset/cleaned_2022dataset_v1.pkl',\n",
    "                  from_scratch=False,\n",
    "                  split_sizes=[21282,2644,2492])\n",
    "dataset.build()\n",
    "\n",
    "X_train = dataset.get_features(split_type=\"train\")\n",
    "Y_train = dataset.get_labels(split_type=\"train\")\n",
    "X_val = dataset.get_features(split_type=\"val\")\n",
    "Y_val = dataset.get_labels(split_type=\"val\")\n",
    "X_test = dataset.get_features(split_type=\"test\")\n",
    "Y_test = dataset.get_labels(split_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c32bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_val_vectorized = vectorizer.transform(X_val)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67aa981",
   "metadata": {},
   "source": [
    "## Train the XGBoost model on the 2016+2022 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56546fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def create_objective(X_train, Y_train, X_test, Y_test):\n",
    "    def objective(trial):\n",
    "        # Suggest values for the hyperparameters\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "\n",
    "        # Create an XGBoost classifier model with suggested parameters\n",
    "        model = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            n_estimators=n_estimators,    # Number of trees\n",
    "            learning_rate=learning_rate,  # Learning rate\n",
    "            max_depth=max_depth,          # Depth of the trees\n",
    "            subsample=subsample,          # Subsampling of the training instances\n",
    "            colsample_bytree=colsample_bytree,  # Subsampling of columns for each tree\n",
    "            seed=RANDOM_SEED,             # Seed for reproducibility\n",
    "            use_label_encoder=False,      # Disable label encoder warning\n",
    "            eval_metric=\"logloss\")\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        # Predict the labels on the test set\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        accuracy = accuracy_score(Y_test, Y_pred)\n",
    "        return accuracy\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e2efd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-09 18:06:27,338] A new study created in memory with name: XGBoost_Pipeline\n",
      "[I 2024-06-09 18:07:10,994] Trial 3 finished with value: 0.7651285930408472 and parameters: {'n_estimators': 211, 'learning_rate': 0.006186040177201665, 'max_depth': 4, 'subsample': 0.8451707562957096, 'colsample_bytree': 0.5711144537871977}. Best is trial 3 with value: 0.7651285930408472.\n",
      "[I 2024-06-09 18:07:36,728] Trial 5 finished with value: 0.7757186081694403 and parameters: {'n_estimators': 120, 'learning_rate': 0.007022088244742499, 'max_depth': 7, 'subsample': 0.5277965457069388, 'colsample_bytree': 0.9675031619290091}. Best is trial 5 with value: 0.7757186081694403.\n",
      "[I 2024-06-09 18:11:40,340] Trial 7 finished with value: 0.8018154311649016 and parameters: {'n_estimators': 133, 'learning_rate': 0.002809967631056339, 'max_depth': 18, 'subsample': 0.5719656726304068, 'colsample_bytree': 0.8781240032230213}. Best is trial 7 with value: 0.8018154311649016.\n",
      "[I 2024-06-09 18:16:06,467] Trial 1 finished with value: 0.7999243570347958 and parameters: {'n_estimators': 346, 'learning_rate': 0.001994008813455999, 'max_depth': 13, 'subsample': 0.796428683071035, 'colsample_bytree': 0.5601339401748893}. Best is trial 7 with value: 0.8018154311649016.\n",
      "[I 2024-06-09 18:18:39,147] Trial 0 finished with value: 0.7934947049924357 and parameters: {'n_estimators': 708, 'learning_rate': 0.0031395612610091994, 'max_depth': 9, 'subsample': 0.6932885767064905, 'colsample_bytree': 0.6619730856012569}. Best is trial 7 with value: 0.8018154311649016.\n",
      "[I 2024-06-09 18:21:21,932] Trial 12 finished with value: 0.7673978819969742 and parameters: {'n_estimators': 619, 'learning_rate': 0.005624676397585353, 'max_depth': 3, 'subsample': 0.5943760034275891, 'colsample_bytree': 0.9323306303876191}. Best is trial 7 with value: 0.8018154311649016.\n",
      "[I 2024-06-09 18:23:03,153] Trial 11 finished with value: 0.7821482602118003 and parameters: {'n_estimators': 503, 'learning_rate': 0.005687400366845753, 'max_depth': 6, 'subsample': 0.9429526143241933, 'colsample_bytree': 0.9740720800575213}. Best is trial 7 with value: 0.8018154311649016.\n",
      "[I 2024-06-09 18:23:27,687] Trial 4 finished with value: 0.7938729198184569 and parameters: {'n_estimators': 783, 'learning_rate': 0.001093138503931645, 'max_depth': 10, 'subsample': 0.7155806412251176, 'colsample_bytree': 0.588361770063476}. Best is trial 7 with value: 0.8018154311649016.\n",
      "[I 2024-06-09 18:24:43,758] Trial 14 finished with value: 0.7628593040847201 and parameters: {'n_estimators': 399, 'learning_rate': 0.0020213778082561373, 'max_depth': 4, 'subsample': 0.77632188118342, 'colsample_bytree': 0.6865622713291096}. Best is trial 7 with value: 0.8018154311649016.\n",
      "[I 2024-06-09 18:27:31,005] Trial 10 finished with value: 0.806732223903177 and parameters: {'n_estimators': 272, 'learning_rate': 0.004211616203699572, 'max_depth': 20, 'subsample': 0.8103138741750413, 'colsample_bytree': 0.5725142695901799}. Best is trial 10 with value: 0.806732223903177.\n",
      "[I 2024-06-09 18:28:20,891] Trial 9 finished with value: 0.794251134644478 and parameters: {'n_estimators': 664, 'learning_rate': 0.0011598004337088593, 'max_depth': 12, 'subsample': 0.636755950872121, 'colsample_bytree': 0.8095123312352099}. Best is trial 10 with value: 0.806732223903177.\n",
      "[I 2024-06-09 18:29:37,645] Trial 16 finished with value: 0.790090771558245 and parameters: {'n_estimators': 258, 'learning_rate': 0.003885237106572024, 'max_depth': 9, 'subsample': 0.5785969505849853, 'colsample_bytree': 0.5860273716833448}. Best is trial 10 with value: 0.806732223903177.\n",
      "[I 2024-06-09 18:47:23,396] Trial 19 finished with value: 0.8014372163388804 and parameters: {'n_estimators': 130, 'learning_rate': 0.009269389275926878, 'max_depth': 20, 'subsample': 0.9009906288399157, 'colsample_bytree': 0.8374561416713668}. Best is trial 10 with value: 0.806732223903177.\n",
      "[I 2024-06-09 18:52:07,533] Trial 15 finished with value: 0.7976550680786687 and parameters: {'n_estimators': 427, 'learning_rate': 0.0018235717434137613, 'max_depth': 17, 'subsample': 0.7110938903810633, 'colsample_bytree': 0.8683188886009657}. Best is trial 10 with value: 0.806732223903177.\n",
      "[I 2024-06-09 18:53:11,859] Trial 8 finished with value: 0.819213313161876 and parameters: {'n_estimators': 997, 'learning_rate': 0.007383049587172839, 'max_depth': 19, 'subsample': 0.8022686274988682, 'colsample_bytree': 0.7884269439802678}. Best is trial 8 with value: 0.819213313161876.\n",
      "[I 2024-06-09 18:53:59,940] Trial 13 finished with value: 0.7980332829046899 and parameters: {'n_estimators': 664, 'learning_rate': 0.0006555327011621787, 'max_depth': 14, 'subsample': 0.908340275644764, 'colsample_bytree': 0.6653338277698122}. Best is trial 8 with value: 0.819213313161876.\n",
      "[I 2024-06-09 18:55:23,702] Trial 2 finished with value: 0.7991679273827534 and parameters: {'n_estimators': 851, 'learning_rate': 0.0007661915390561898, 'max_depth': 19, 'subsample': 0.5280692834565781, 'colsample_bytree': 0.8508358311737453}. Best is trial 8 with value: 0.819213313161876.\n",
      "[I 2024-06-09 18:56:14,366] Trial 6 finished with value: 0.8074886535552194 and parameters: {'n_estimators': 837, 'learning_rate': 0.004051804972159528, 'max_depth': 19, 'subsample': 0.8855473243937781, 'colsample_bytree': 0.9703842265966671}. Best is trial 8 with value: 0.819213313161876.\n",
      "[I 2024-06-09 18:58:56,607] Trial 17 finished with value: 0.8199697428139183 and parameters: {'n_estimators': 943, 'learning_rate': 0.00920796395393293, 'max_depth': 20, 'subsample': 0.976996041734845, 'colsample_bytree': 0.7861283006119602}. Best is trial 17 with value: 0.8199697428139183.\n",
      "[I 2024-06-09 19:00:19,585] Trial 18 finished with value: 0.8139183055975794 and parameters: {'n_estimators': 938, 'learning_rate': 0.0040217993990136055, 'max_depth': 20, 'subsample': 0.9058935206920903, 'colsample_bytree': 0.8161186749477821}. Best is trial 17 with value: 0.8199697428139183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.8199697428139183\n",
      "Best hyperparameters: {'n_estimators': 943, 'learning_rate': 0.00920796395393293, 'max_depth': 20, 'subsample': 0.976996041734845, 'colsample_bytree': 0.7861283006119602}\n"
     ]
    }
   ],
   "source": [
    "# Create a study object\n",
    "# study = optuna.create_study(direction=\"maximize\", study_name=f\"LDA_XGBoost_Pipeline\")\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=f\"XGBoost_Pipeline\")\n",
    "\n",
    "\n",
    "# Create the study objective\n",
    "objective = create_objective(X_train_vectorized, Y_train, X_val_vectorized, Y_val)\n",
    "\n",
    "# Execute an optimization\n",
    "study.optimize(objective, n_trials=20, n_jobs=-1)\n",
    "\n",
    "# Print the best trial results\n",
    "print(f\"Best Accuracy: {study.best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42ac769e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Model Accuracy: 0.8252818035426731\n",
      "Precision: 0.813076923076923\n",
      "Recall: 0.8469551282051282\n",
      "F1 Score: 0.8296703296703297\n"
     ]
    }
   ],
   "source": [
    "best_n_estimators = study.best_trial.params[\"n_estimators\"]\n",
    "best_learning_rate = study.best_trial.params[\"learning_rate\"]\n",
    "best_max_depth = study.best_trial.params[\"max_depth\"]\n",
    "best_subsample = study.best_trial.params[\"subsample\"]\n",
    "best_colsample_bytree = study.best_trial.params[\"colsample_bytree\"]\n",
    "\n",
    "model = xgb.XGBClassifier(n_estimators=best_n_estimators, \n",
    "                                 learning_rate=best_learning_rate,\n",
    "                                 max_depth=best_max_depth,\n",
    "                                 random_state=RANDOM_SEED)\n",
    "model.fit(X_train_vectorized, Y_train)\n",
    "Y_pred = model.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "precision = precision_score(Y_test, Y_pred)\n",
    "recall = recall_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Best XGBoost Model Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa78914",
   "metadata": {},
   "source": [
    "Training Time: 53m 52s \\\n",
    "Inference Time: 6m 19s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
