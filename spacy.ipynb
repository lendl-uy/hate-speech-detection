{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing spaCy's Text Categorizer for Classifying Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from dataset.dataset import Dataset\n",
    "from constants import *\n",
    "\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from dataset/cleaned_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "dataset = Dataset(full_data_path=CLEANED_DATASET_PATH,\n",
    "                  from_scratch=False,\n",
    "                  split_sizes=split_sizes_cleaned)\n",
    "dataset.build()\n",
    "\n",
    "X_train = dataset.get_features(split_type=\"train\")\n",
    "Y_train = dataset.get_labels(split_type=\"train\")\n",
    "X_val = dataset.get_features(split_type=\"val\")\n",
    "Y_val = dataset.get_labels(split_type=\"val\")\n",
    "X_test = dataset.get_features(split_type=\"test\")\n",
    "Y_test = dataset.get_labels(split_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_train and Y_train into a list of dictionaries\n",
    "data = [{\"text\": tweet, \"label\": label} for tweet, label in zip(X_train, Y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate an empty spacy model\n",
    "nlp = spacy.blank(\"en-tl\")\n",
    "\n",
    "# Add a text categorizer to the pipeline\n",
    "if \"textcat\" not in nlp.pipe_names:\n",
    "    textcat = nlp.add_pipe(\"textcat\", last=True)\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "# Add labels to the text classifier\n",
    "textcat.add_label(\"HATE_SPEECH\")\n",
    "textcat.add_label(\"NOT_HATE_SPEECH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to spaCy's Example format\n",
    "examples = []\n",
    "for entry in data:\n",
    "    doc = nlp.make_doc(entry[\"text\"])\n",
    "    cats = {\"HATE_SPEECH\": entry[\"label\"] == 1, \n",
    "            \"NOT_HATE_SPEECH\": entry[\"label\"] == 0}\n",
    "    examples.append(Example.from_dict(doc, {\"cats\": cats}))\n",
    "\n",
    "# Convert examples to DocBin for efficient serialization\n",
    "train_docbin = DocBin(docs=[example.reference for example in examples])\n",
    "train_docbin.to_disk(\"dataset/train.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: {'textcat': 288.25427383184433}\n",
      "Iteration 1, Loss: {'textcat': 245.54165340494365}\n",
      "Iteration 2, Loss: {'textcat': 216.3791906798724}\n",
      "Iteration 3, Loss: {'textcat': 191.04857891355277}\n",
      "Iteration 4, Loss: {'textcat': 171.08024123770156}\n",
      "Iteration 5, Loss: {'textcat': 159.13212397720417}\n",
      "Iteration 6, Loss: {'textcat': 149.4866166453976}\n",
      "Iteration 7, Loss: {'textcat': 138.31153382081493}\n",
      "Iteration 8, Loss: {'textcat': 129.21508954725687}\n",
      "Iteration 9, Loss: {'textcat': 121.1759618195199}\n",
      "Training Time: 88.0280921459198\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "def train_model(examples, iterations):\n",
    "    start = time()\n",
    "    optimizer = nlp.initialize()\n",
    "    for i in range(iterations):\n",
    "        random.shuffle(examples)\n",
    "        losses = {}\n",
    "        # Batch the examples and iterate over them\n",
    "        batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            nlp.update(batch, drop=0.5, sgd=optimizer, losses=losses)\n",
    "        print(f\"Iteration {i}, Loss: {losses}\")\n",
    "    end = time()\n",
    "    print(f'Training Time: {end-start}')\n",
    "\n",
    "# Train the model for a given number of iterations\n",
    "train_model(examples, iterations=10)\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk(\"models/textcat_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7364185110663984\n",
      "Precision: 0.7385691231845078\n",
      "Recall: 0.7095607235142118\n",
      "F1-Score: 0.7237743806009489\n",
      "Inference Time: 2.350472927093506\n"
     ]
    }
   ],
   "source": [
    "# Function to get predictions from the model\n",
    "def get_predictions(nlp, texts):\n",
    "    predictions = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        # Get the label with the highest score\n",
    "        if doc.cats[\"HATE_SPEECH\"] > doc.cats[\"NOT_HATE_SPEECH\"]:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n",
    "\n",
    "start = time()\n",
    "# Get predictions for X_test\n",
    "y_pred = get_predictions(nlp, X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "precision = precision_score(Y_test, y_pred)\n",
    "recall = recall_score(Y_test, y_pred)\n",
    "f1 = f1_score(Y_test, y_pred)\n",
    "end = time()\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f'Inference Time: {end-start}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
