{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d05c036",
   "metadata": {},
   "source": [
    "# LDA - RF Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778cef84980d4467",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:12:15.723490Z",
     "start_time": "2024-04-30T06:12:14.266895Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import optuna\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from dataset.dataset import Dataset\n",
    "from dataset.dataset_constants import split_sizes_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ffafa63beef650",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e92953c8b90f034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:12:15.730647Z",
     "start_time": "2024-04-30T06:12:15.723831Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be051016d14124",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load the Hate Speech Filipino dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b506f040054ff5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:12:25.180185Z",
     "start_time": "2024-04-30T06:12:16.265426Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from dataset/cleaned_dataset_v1.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(full_data_path=\"dataset/cleaned_dataset_v1.pkl\",\n",
    "                  from_scratch=False,\n",
    "                  split_sizes=split_sizes_cleaned)\n",
    "dataset.build()\n",
    "\n",
    "X_train = dataset.get_features(split_type=\"train\")\n",
    "Y_train = dataset.get_labels(split_type=\"train\")\n",
    "X_val = dataset.get_features(split_type=\"val\")\n",
    "Y_val = dataset.get_labels(split_type=\"val\")\n",
    "X_test = dataset.get_features(split_type=\"test\")\n",
    "Y_test = dataset.get_labels(split_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76bd2607ff0e948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:12:25.205936Z",
     "start_time": "2024-04-30T06:12:25.196203Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-split the dataset into training, validation, and test sets\n",
    "# X = X_train + X_val + X_test\n",
    "# Y = Y_train + Y_val + Y_test\n",
    "# X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8c684144c6b8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Vectorize the texts to be able to perform LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a447fade33a0d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:12:25.321426Z",
     "start_time": "2024-04-30T06:12:25.219275Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "X_train_vector = vectorizer.fit_transform(X_train)\n",
    "X_val_counts = vectorizer.transform(X_val)\n",
    "X_test_counts = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6269a60",
   "metadata": {},
   "source": [
    "## Search for the best hyperparameters of the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943d4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def create_objective(X_train, Y_train, X_test, Y_test):\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "        rf = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                    random_state=RANDOM_SEED)\n",
    "        rf.fit(X_train, Y_train)\n",
    "        Y_pred = rf.predict(X_test)\n",
    "        score = accuracy_score(Y_test, Y_pred)\n",
    "        return score\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff871c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-14 15:17:46,959] A new study created in memory with name: RF_Pipeline\n",
      "[I 2024-06-14 15:18:14,993] Trial 7 finished with value: 0.7326257265605256 and parameters: {'n_estimators': 422}. Best is trial 7 with value: 0.7326257265605256.\n",
      "[I 2024-06-14 15:18:37,407] Trial 1 finished with value: 0.731614859742229 and parameters: {'n_estimators': 761}. Best is trial 7 with value: 0.7326257265605256.\n",
      "[I 2024-06-14 15:18:39,330] Trial 0 finished with value: 0.731614859742229 and parameters: {'n_estimators': 804}. Best is trial 7 with value: 0.7326257265605256.\n",
      "[I 2024-06-14 15:18:41,066] Trial 6 finished with value: 0.7323730098559514 and parameters: {'n_estimators': 813}. Best is trial 7 with value: 0.7326257265605256.\n",
      "[I 2024-06-14 15:18:41,775] Trial 5 finished with value: 0.7318675764468031 and parameters: {'n_estimators': 828}. Best is trial 7 with value: 0.7326257265605256.\n",
      "[I 2024-06-14 15:18:44,122] Trial 2 finished with value: 0.7321202931513773 and parameters: {'n_estimators': 867}. Best is trial 7 with value: 0.7326257265605256.\n",
      "[I 2024-06-14 15:18:45,480] Trial 3 finished with value: 0.7318675764468031 and parameters: {'n_estimators': 887}. Best is trial 7 with value: 0.7326257265605256.\n",
      "[I 2024-06-14 15:18:47,854] Trial 4 finished with value: 0.7308567096285065 and parameters: {'n_estimators': 926}. Best is trial 7 with value: 0.7326257265605256.\n",
      "[I 2024-06-14 15:18:51,792] Trial 9 finished with value: 0.7333838766742482 and parameters: {'n_estimators': 219}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:18:55,007] Trial 13 finished with value: 0.731614859742229 and parameters: {'n_estimators': 170}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:18:55,988] Trial 8 finished with value: 0.7306039929239323 and parameters: {'n_estimators': 626}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:18:58,884] Trial 10 finished with value: 0.733131159969674 and parameters: {'n_estimators': 303}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:19:03,354] Trial 12 finished with value: 0.7311094263330806 and parameters: {'n_estimators': 342}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:19:04,675] Trial 17 finished with value: 0.7318675764468031 and parameters: {'n_estimators': 155}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:19:09,971] Trial 19 finished with value: 0.7313621430376548 and parameters: {'n_estimators': 186}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:19:11,529] Trial 16 finished with value: 0.7323730098559514 and parameters: {'n_estimators': 328}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:19:11,648] Trial 18 finished with value: 0.7321202931513773 and parameters: {'n_estimators': 269}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:19:13,798] Trial 15 finished with value: 0.7313621430376548 and parameters: {'n_estimators': 438}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:19:15,266] Trial 11 finished with value: 0.7311094263330806 and parameters: {'n_estimators': 583}. Best is trial 9 with value: 0.7333838766742482.\n",
      "[I 2024-06-14 15:19:20,923] Trial 14 finished with value: 0.7313621430376548 and parameters: {'n_estimators': 691}. Best is trial 9 with value: 0.7333838766742482.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.7333838766742482\n",
      "Best hyperparameters: {'n_estimators': 219}\n"
     ]
    }
   ],
   "source": [
    "# Create a study object\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=f\"RF_Pipeline\")\n",
    "\n",
    "# Create the study objective\n",
    "objective = create_objective(X_train_vector, Y_train, X_val_counts, Y_val)\n",
    "\n",
    "# Execute an optimization\n",
    "study.optimize(objective, n_trials=20, n_jobs=-1)\n",
    "\n",
    "# Print the best trial results\n",
    "print(f\"Best Accuracy: {study.best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fce6536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best NMF-RF Model Accuracy: 0.7174920712368871\n"
     ]
    }
   ],
   "source": [
    "best_n_estimators = study.best_trial.params[\"n_estimators\"]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=best_n_estimators, random_state=RANDOM_SEED)\n",
    "rf.fit(X_train_vector, Y_train)\n",
    "Y_pred = rf.predict(X_test_counts)\n",
    "score = accuracy_score(Y_test, Y_pred)\n",
    "# print(f\"Best LDA-RF Model Accuracy: {score}\")\n",
    "print(f\"Best NMF-RF Model Accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5148cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def create_objective(X_train, Y_train, X_test, Y_test):\n",
    "    def objective(trial):\n",
    "        # Suggest values for the hyperparameters\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "\n",
    "        # Create an XGBoost classifier model with suggested parameters\n",
    "        model = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            n_estimators=n_estimators,    # Number of trees\n",
    "            learning_rate=learning_rate,  # Learning rate\n",
    "            max_depth=max_depth,          # Depth of the trees\n",
    "            subsample=subsample,          # Subsampling of the training instances\n",
    "            colsample_bytree=colsample_bytree,  # Subsampling of columns for each tree\n",
    "            seed=RANDOM_SEED,             # Seed for reproducibility\n",
    "            use_label_encoder=False,      # Disable label encoder warning\n",
    "            eval_metric=\"logloss\")\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        # Predict the labels on the test set\n",
    "        Y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        accuracy = accuracy_score(Y_test, Y_pred)\n",
    "        return accuracy\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d79ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-14 15:22:28,991] A new study created in memory with name: XGBoost_Pipeline\n",
      "[I 2024-06-14 15:22:33,513] Trial 7 finished with value: 0.6413949962092494 and parameters: {'n_estimators': 266, 'learning_rate': 0.00481807085623704, 'max_depth': 7, 'subsample': 0.7050784296785573, 'colsample_bytree': 0.9642485952958153}. Best is trial 7 with value: 0.6413949962092494.\n",
      "[I 2024-06-14 15:22:33,915] Trial 4 finished with value: 0.6499873641647713 and parameters: {'n_estimators': 121, 'learning_rate': 0.002419082798583541, 'max_depth': 17, 'subsample': 0.9159316590587236, 'colsample_bytree': 0.6601483126580272}. Best is trial 4 with value: 0.6499873641647713.\n",
      "[I 2024-06-14 15:22:36,452] Trial 1 finished with value: 0.6105635582512005 and parameters: {'n_estimators': 487, 'learning_rate': 0.0008392042108450673, 'max_depth': 5, 'subsample': 0.9363021485921652, 'colsample_bytree': 0.6566180999905978}. Best is trial 4 with value: 0.6499873641647713.\n",
      "[I 2024-06-14 15:22:43,871] Trial 5 finished with value: 0.6403841293909528 and parameters: {'n_estimators': 593, 'learning_rate': 0.0006933442097880314, 'max_depth': 10, 'subsample': 0.6520958720813692, 'colsample_bytree': 0.563657065885654}. Best is trial 4 with value: 0.6499873641647713.\n",
      "[I 2024-06-14 15:22:44,442] Trial 8 finished with value: 0.6664139499620925 and parameters: {'n_estimators': 268, 'learning_rate': 0.0027744912792160565, 'max_depth': 16, 'subsample': 0.7612574994895409, 'colsample_bytree': 0.5567896975337433}. Best is trial 8 with value: 0.6664139499620925.\n",
      "[I 2024-06-14 15:22:44,914] Trial 0 finished with value: 0.6204195097295931 and parameters: {'n_estimators': 761, 'learning_rate': 0.0005345369274096805, 'max_depth': 8, 'subsample': 0.6632694757383166, 'colsample_bytree': 0.9231220076206919}. Best is trial 8 with value: 0.6664139499620925.\n",
      "[I 2024-06-14 15:22:48,327] Trial 9 finished with value: 0.6383623957543594 and parameters: {'n_estimators': 476, 'learning_rate': 0.0010585579996609813, 'max_depth': 11, 'subsample': 0.5406933007003132, 'colsample_bytree': 0.9975617813650289}. Best is trial 8 with value: 0.6664139499620925.\n",
      "[I 2024-06-14 15:22:48,403] Trial 13 finished with value: 0.6464493303007329 and parameters: {'n_estimators': 105, 'learning_rate': 0.007770175787294905, 'max_depth': 10, 'subsample': 0.5184184300113694, 'colsample_bytree': 0.8903121702995977}. Best is trial 8 with value: 0.6664139499620925.\n",
      "[I 2024-06-14 15:22:49,712] Trial 6 finished with value: 0.6896638867829163 and parameters: {'n_estimators': 966, 'learning_rate': 0.0044199680784754135, 'max_depth': 8, 'subsample': 0.7212212870712782, 'colsample_bytree': 0.7959813747943758}. Best is trial 6 with value: 0.6896638867829163.\n",
      "[I 2024-06-14 15:22:52,163] Trial 2 finished with value: 0.7083649229214051 and parameters: {'n_estimators': 884, 'learning_rate': 0.009910663409250252, 'max_depth': 10, 'subsample': 0.7488232999833895, 'colsample_bytree': 0.6401077504242172}. Best is trial 2 with value: 0.7083649229214051.\n",
      "[I 2024-06-14 15:22:52,547] Trial 16 finished with value: 0.6429112964366944 and parameters: {'n_estimators': 124, 'learning_rate': 0.00825213571631991, 'max_depth': 8, 'subsample': 0.5899675172239096, 'colsample_bytree': 0.8183809388649331}. Best is trial 2 with value: 0.7083649229214051.\n",
      "[I 2024-06-14 15:22:54,756] Trial 3 finished with value: 0.6866312863280263 and parameters: {'n_estimators': 527, 'learning_rate': 0.0033730621496661996, 'max_depth': 19, 'subsample': 0.8995919921232061, 'colsample_bytree': 0.8091244837486566}. Best is trial 2 with value: 0.7083649229214051.\n",
      "[I 2024-06-14 15:22:57,856] Trial 10 finished with value: 0.7065959059893859 and parameters: {'n_estimators': 595, 'learning_rate': 0.009375301190085415, 'max_depth': 14, 'subsample': 0.5968912073662247, 'colsample_bytree': 0.9205419493563881}. Best is trial 2 with value: 0.7083649229214051.\n",
      "[I 2024-06-14 15:23:00,283] Trial 11 finished with value: 0.6810715188273945 and parameters: {'n_estimators': 376, 'learning_rate': 0.0044798287150282804, 'max_depth': 16, 'subsample': 0.8402192827134833, 'colsample_bytree': 0.729134784858863}. Best is trial 2 with value: 0.7083649229214051.\n",
      "[I 2024-06-14 15:23:00,947] Trial 12 finished with value: 0.6621177659843316 and parameters: {'n_estimators': 325, 'learning_rate': 0.0018899764329050079, 'max_depth': 19, 'subsample': 0.7874408030383165, 'colsample_bytree': 0.833034884339446}. Best is trial 2 with value: 0.7083649229214051.\n",
      "[I 2024-06-14 15:23:01,254] Trial 18 finished with value: 0.6696992671215567 and parameters: {'n_estimators': 985, 'learning_rate': 0.0062643037133691525, 'max_depth': 3, 'subsample': 0.8068235197100725, 'colsample_bytree': 0.7360807226776201}. Best is trial 2 with value: 0.7083649229214051.\n",
      "[I 2024-06-14 15:23:02,999] Trial 19 finished with value: 0.6851149861005813 and parameters: {'n_estimators': 977, 'learning_rate': 0.009902681238214778, 'max_depth': 3, 'subsample': 0.8061716783495472, 'colsample_bytree': 0.6854035092308574}. Best is trial 2 with value: 0.7083649229214051.\n",
      "[I 2024-06-14 15:23:04,093] Trial 14 finished with value: 0.6113217083649229 and parameters: {'n_estimators': 939, 'learning_rate': 0.0002314690866569347, 'max_depth': 7, 'subsample': 0.8807968381461113, 'colsample_bytree': 0.8889597518622183}. Best is trial 2 with value: 0.7083649229214051.\n",
      "[I 2024-06-14 15:23:06,482] Trial 15 finished with value: 0.7030578721253475 and parameters: {'n_estimators': 826, 'learning_rate': 0.0059297197603076345, 'max_depth': 14, 'subsample': 0.6886858222903476, 'colsample_bytree': 0.7035804418784329}. Best is trial 2 with value: 0.7083649229214051.\n",
      "[I 2024-06-14 15:23:08,876] Trial 17 finished with value: 0.7210007581501138 and parameters: {'n_estimators': 964, 'learning_rate': 0.009929167613734993, 'max_depth': 20, 'subsample': 0.8376826351129252, 'colsample_bytree': 0.733864778451071}. Best is trial 17 with value: 0.7210007581501138.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.7210007581501138\n",
      "Best hyperparameters: {'n_estimators': 964, 'learning_rate': 0.009929167613734993, 'max_depth': 20, 'subsample': 0.8376826351129252, 'colsample_bytree': 0.733864778451071}\n"
     ]
    }
   ],
   "source": [
    "# Create a study object\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=f\"XGBoost_Pipeline\")\n",
    "\n",
    "# Create the study objective\n",
    "objective = create_objective(X_train_vector, Y_train, X_val_counts, Y_val)\n",
    "\n",
    "# Execute an optimization\n",
    "study.optimize(objective, n_trials=20, n_jobs=-1)\n",
    "\n",
    "# Print the best trial results\n",
    "print(f\"Best Accuracy: {study.best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f6d43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best NMF-XGBoost Model Accuracy: 0.7226152720175653\n"
     ]
    }
   ],
   "source": [
    "best_n_estimators = study.best_trial.params[\"n_estimators\"]\n",
    "best_learning_rate = study.best_trial.params[\"learning_rate\"]\n",
    "best_max_depth = study.best_trial.params[\"max_depth\"]\n",
    "best_subsample = study.best_trial.params[\"subsample\"]\n",
    "best_colsample_bytree = study.best_trial.params[\"colsample_bytree\"]\n",
    "\n",
    "model = xgb.XGBClassifier(n_estimators=best_n_estimators, \n",
    "                                 learning_rate=best_learning_rate,\n",
    "                                 max_depth=best_max_depth,\n",
    "                                 random_state=RANDOM_SEED)\n",
    "model.fit(X_train_vector, Y_train)\n",
    "Y_pred = model.predict(X_test_counts)\n",
    "score = accuracy_score(Y_test, Y_pred)\n",
    "# print(f\"Best LDA-XGBoost Model Accuracy: {score}\")\n",
    "print(f\"Best NMF-XGBoost Model Accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac9ef1d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autosklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mautosklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define Auto-sklearn classifier with validation set\u001b[39;00m\n\u001b[1;32m      4\u001b[0m automl \u001b[38;5;241m=\u001b[39m autosklearn\u001b[38;5;241m.\u001b[39mclassification\u001b[38;5;241m.\u001b[39mAutoSklearnClassifier(\n\u001b[1;32m      5\u001b[0m     time_left_for_this_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m,\n\u001b[1;32m      6\u001b[0m     per_run_time_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     resampling_strategy_arguments\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autosklearn'"
     ]
    }
   ],
   "source": [
    "import autosklearn.classification\n",
    "\n",
    "# Define Auto-sklearn classifier with validation set\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=120,\n",
    "    per_run_time_limit=30,\n",
    "    include={\n",
    "        \"classifier\": [\"random_forest\", \"gradient_boosting\", \"xgboost\", \"liblinear_svc\", \"libsvm_svc\"]\n",
    "    },\n",
    "    ensemble_size=1,\n",
    "    n_jobs=-1,\n",
    "    # New: Pass validation data using X_val and y_val \n",
    "    validation_split=0.0,\n",
    "    resampling_strategy='holdout',\n",
    "    resampling_strategy_arguments={'train_size': 0.8, 'shuffle': True},\n",
    ")\n",
    "\n",
    "# Train the models\n",
    "automl.fit(X_train_vector, Y_train, X_val_counts, Y_val) # Pass validation data here\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "Y_pred = automl.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print the best model and its hyperparameters\n",
    "print(\"Best Model:\", automl.show_models())\n",
    "print(\"Best Hyperparameters:\", automl.sprint_statistics())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
