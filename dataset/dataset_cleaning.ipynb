{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from dataset import Dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset_constants) # reload to update changes to the file\n",
    "from dataset_constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(train_path=TRAIN_DATASET_PATH,\n",
    "                  val_path=VALIDATION_DATASET_PATH, \n",
    "                  test_path=TEST_DATASET_PATH)\n",
    "dataset.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Inaasahan na ni Vice President Jejomar Binay na may mga taong... https://t.co/SDytgbWiLh', 'Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA NGA DI STRAIGHT', 'Salamat sa walang sawang suporta ng mga taga makati! Ang Pagbabalik Binay In Makati #OnlyBinayInMakatiSanKaPa https://t.co/iwAOdtZPRE', '@rapplerdotcom putangina mo binay TAKBO PA', 'Binay with selective amnesia, forgetting about the past six years he spent preparing to be president.  #PiliPinasDebates2016']\n",
      "[0, 1, 0, 1, 0]\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_train = dataset.get_features(split_type=\"train\")\n",
    "Y_train = dataset.get_labels(split_type=\"train\")\n",
    "\n",
    "print(X_train[:5])\n",
    "print(Y_train[:5])\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Escudero denies betraying Poe after meeting with Binay |https://t.co/sKlXTIhHJa - Kare-kare at sinampalukan ang topic. Walang balimbing?', 'Hndi ko makita yung sa one more chance saka kay binay sa fb. Haist.', \"Mar Roxas is now addressing the crowd gathered at Pasay City's Ulat sa Barangay 2016  https://t.co/VruZyJ2e2H\", '@ImYourBaeMax perfect! Para makaharap ni Duterte ang mga Binay at makatikim ng mura #^%* i#*', '#OnlyBinayPriority4Ps Wag nating hayaan na maloko tayo ng mga pulitikong yan. Kay Binay na tayo']\n",
      "[0, 1, 0, 0, 0]\n",
      "4232\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "X_val = dataset.get_features(split_type=\"val\")\n",
    "Y_val = dataset.get_labels(split_type=\"val\")\n",
    "\n",
    "print(X_val[:5])\n",
    "print(Y_val[:5])\n",
    "print(len(X_val))\n",
    "print(len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unshaded votes and votes for Mayor Duterte goes to Mar Roxas according to some reports of ballot tests.  #AyawSaDILAW', '#NoMoreChance https://t.co/msaaUGv0bS', \"@itsmanj well there's other good choices like Duterte or Poe. But both of them are still undecided, I think? :( :(\", 'Nognog. Pandak. Laki sa hirap. Pero corrupt. Yan si Binay!!!', 'Ex-Binay aide turns tables on Mercado | https://t.co/nyySAo54rL']\n",
      "[1, 1, 0, 1, 0]\n",
      "4232\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "X_test = dataset.get_features(split_type=\"test\")\n",
    "Y_test = dataset.get_labels(split_type=\"test\")\n",
    "\n",
    "print(X_test[:5])\n",
    "print(Y_test[:5])\n",
    "print(len(X_test))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18464\n",
      "18464\n",
      "['Inaasahan na ni Vice President Jejomar Binay na may mga taong... https://t.co/SDytgbWiLh', 'Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA NGA DI STRAIGHT', 'Salamat sa walang sawang suporta ng mga taga makati! Ang Pagbabalik Binay In Makati #OnlyBinayInMakatiSanKaPa https://t.co/iwAOdtZPRE', '@rapplerdotcom putangina mo binay TAKBO PA', 'Binay with selective amnesia, forgetting about the past six years he spent preparing to be president.  #PiliPinasDebates2016']\n",
      "[0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "X = dataset.get_features()\n",
    "Y = dataset.get_labels()\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "print(X[:5])\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_texts(texts, pattern_type):\n",
    "    # Define the regex pattern\n",
    "    if pattern_type == 'url':\n",
    "        pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    elif pattern_type == 'username':\n",
    "        pattern = r'@\\S+'\n",
    "    elif pattern_type == 'numeric':\n",
    "        pattern = r'\\b\\d+\\b'\n",
    "    elif pattern_type == 'html_tags':\n",
    "        pattern = r'<.*?>+'\n",
    "    elif pattern_type == 'newlines':\n",
    "        pattern = r'\\n'\n",
    "    elif pattern_type == 'punctuation':\n",
    "        pattern = '[%s]' % re.escape(string.punctuation)\n",
    "    elif pattern_type == 'emoji':\n",
    "        pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               \"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each text and check for the presence of the pattern\n",
    "    for text in texts:\n",
    "        if re.search(pattern, text):\n",
    "            count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(texts, pattern_type):\n",
    "    # Define the regex pattern\n",
    "    if pattern_type == 'url':\n",
    "        pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    elif pattern_type == 'username':\n",
    "        pattern = r'@\\S+'\n",
    "    elif pattern_type == 'numeric':\n",
    "        pattern = r'\\b\\d+\\b'\n",
    "    elif pattern_type == 'html_tags':\n",
    "        pattern = r'<.*?>+'\n",
    "    elif pattern_type == 'newlines':\n",
    "        pattern = r'\\n'\n",
    "    elif pattern_type == 'punctuation':\n",
    "        pattern = '[%s]' % re.escape(string.punctuation)\n",
    "\n",
    "\n",
    "    # Remove URLs from each text in the list\n",
    "    return [re.sub(pattern, '', text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_contractions = {\n",
    "    \"gov't\": \"government\",\n",
    "    \"s'ya\": \"siya\",\n",
    "    \"sa'yo\": \"sa iyo\",\n",
    "    \"ika'y\": \"ikaw ay\",\n",
    "    \"everybody's\": \"everybody is\",\n",
    "    \"mo'ko\": \"mo ako\",\n",
    "    \"ba't\": \"bakit\",\n",
    "    \"sila'y\": \"sila ay\",\n",
    "    \"aba'y\": \"aba ay\",\n",
    "    \"ito'y\": \"ito ay\",\n",
    "    \"mgm't\": \"management\",\n",
    "    \"shut'up\": \"shut up\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"umano'y\": \"umano ay\",\n",
    "    \"kaya't\": \"kaya at\",\n",
    "    \"n'ya\": \"niya\",\n",
    "    \"le'me\": \"let me\",\n",
    "    \"c'mon\": \"common\",\n",
    "    \"isa't\": \"isa at\",\n",
    "    \"ako'y\": \"ako ay\",\n",
    "    \"toyo't\": \"toyo at\",\n",
    "    \"na'to\": \"na ito\",\n",
    "    \"n'yo\": \"niyo\"\n",
    "}\n",
    "\n",
    "def expand_all_contractions(text, custom_dict):\n",
    "    # First, expand using the default package\n",
    "    expanded_text = contractions.fix(text)\n",
    "    # Now apply custom contractions\n",
    "    for key, value in custom_dict.items():\n",
    "        expanded_text = expanded_text.replace(key, value)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_contractions(texts):\n",
    "    # Regular expression to match contractions\n",
    "    contraction_pattern = r\"\\b\\w+['â€™]\\w+\\b\"\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each text and count contractions\n",
    "    for text in texts:\n",
    "        # Find all instances of the pattern\n",
    "        contracted_words = re.findall(contraction_pattern, text)\n",
    "        count += len(contracted_words)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read stopwords from file and save into a list\n",
    "def read_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords_list = [word.strip() for word in file.readlines()]\n",
    "    return stopwords_list\n",
    "\n",
    "# Read English stopwords\n",
    "english_stopwords = read_stopwords(STOPWORDS_ENGLISH_PATH)\n",
    "\n",
    "# Read Tagalog stopwords\n",
    "tagalog_stopwords = read_stopwords(STOPWORDS_TAGALOG_PATH)\n",
    "\n",
    "# Combine stopwords\n",
    "combined_stopwords = set(english_stopwords + tagalog_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Filter out the stopwords\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    # Join the filtered words back into a single string\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(X, y):\n",
    "    # Create a dictionary to store indices of non-unique tweets\n",
    "    non_unique_indices = defaultdict(list)\n",
    "\n",
    "    # Iterate over the feature data X and store the indices of non-unique tweets\n",
    "    for i, tweet in enumerate(X):\n",
    "        non_unique_indices[tweet].append(i)\n",
    "\n",
    "    # Identify the indices of duplicates\n",
    "    duplicate_indices = [indices for indices in non_unique_indices.values() if len(indices) > 1]\n",
    "\n",
    "    # Flatten the list of duplicate indices\n",
    "    duplicate_indices = [idx for sublist in duplicate_indices for idx in sublist]\n",
    "\n",
    "    # Remove duplicates from X and y\n",
    "    X_unique = [X[i] for i in range(len(X)) if i not in duplicate_indices]\n",
    "    y_unique = [y[i] for i in range(len(y)) if i not in duplicate_indices]\n",
    "\n",
    "    # Now, count how many removed items belong to each set\n",
    "    removed_train_count = sum(1 for idx in duplicate_indices if idx < split_sizes[0])\n",
    "    removed_validation_count = sum(1 for idx in duplicate_indices if split_sizes[0] <= idx < split_sizes[0] + split_sizes[1])\n",
    "    removed_test_count = sum(1 for idx in duplicate_indices if split_sizes[0] + split_sizes[1] <= idx)\n",
    "    total_removed = removed_train_count + removed_validation_count + removed_test_count\n",
    "\n",
    "    print(f\"Removed {total_removed} non-unique tweets.\")\n",
    "    print(\"Removed from train set:\", removed_train_count)\n",
    "    print(\"Removed from validation set:\", removed_validation_count)\n",
    "    print(\"Removed from test set:\", removed_test_count)\n",
    "\n",
    "    return X_unique, y_unique, non_unique_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_pipeline(X, Y):\n",
    "    # Remove URLs\n",
    "    print(f\"Text with URLs: {count_texts(X, 'url')}\")\n",
    "    X_url_removed = clean_texts(X, 'url')\n",
    "    print(f\"Removed URLs. New count of text with URLs: {count_texts(X_url_removed, 'url')}\\n\")\n",
    "\n",
    "    # Remove usernames\n",
    "    print(f\"Text with usernames: {count_texts(X_url_removed, 'username')}\")\n",
    "    X_username_removed = clean_texts(X_url_removed, 'username')\n",
    "    print(f\"Removed usernames. New count of text with usernames: {count_texts(X_username_removed, 'username')}\\n\")\n",
    "\n",
    "    # Remove words that are completely numbers\n",
    "    print(f\"Text with numeric words: {count_texts(X_username_removed, 'numeric')}\")\n",
    "    X_numeric_removed = clean_texts(X_username_removed, 'numeric')\n",
    "    print(f\"Removed numeric words. New count of text with numeric words: {count_texts(X_numeric_removed, 'numeric')}\\n\")\n",
    "\n",
    "    # Remove HTML tags\n",
    "    print(f\"Text with HTML tags: {count_texts(X_numeric_removed, 'html_tags')}\")\n",
    "    X_html_removed = clean_texts(X_numeric_removed, 'html_tags')\n",
    "    print(f\"Removed HTML tags. New count of text with HTML tags: {count_texts(X_html_removed, 'html_tags')}\\n\")\n",
    "\n",
    "    # Remove newlines\n",
    "    print(f\"Text with newlines: {count_texts(X_html_removed, 'newlines')}\")\n",
    "    X_newlines_removed = clean_texts(X_html_removed, 'newlines')\n",
    "    print(f\"Removed newlines. New count of text with newlines: {count_texts(X_newlines_removed, 'newlines')}\\n\")\n",
    "\n",
    "    # Lowercase all texts\n",
    "    X_lowercased = [text.lower() for text in X_newlines_removed]\n",
    "    print(f\"Converted all texts to lowercase.\\n\")\n",
    "\n",
    "    # Expand contractions\n",
    "    print(f\"Identified {count_contractions(X_lowercased)} contractions. Attempting to expand some of them.\")\n",
    "    X_expanded = [expand_all_contractions(text, custom_contractions) for text in X_lowercased]\n",
    "    print(f\"Expanded contractions. Remaining contractions: {count_contractions(X_expanded)} (Some are possessive.)\\n\")\n",
    "\n",
    "    # Remove punctuations - CAN CONSIDER NOT REMOVING\n",
    "    print(f\"Text with punctuations: {count_texts(X_expanded, 'punctuation')}\")\n",
    "    X_punctuation_removed = clean_texts(X_expanded, 'punctuation')\n",
    "    print(f\"Removed punctuations. New count of text with punctuations: {count_texts(X_punctuation_removed, 'punctuation')}\\n\")\n",
    "\n",
    "    # Remove stop words - both English and Filipino\n",
    "    X_no_stopwords = [remove_stopwords(text, combined_stopwords) for text in X_punctuation_removed]\n",
    "    print(\"Removed stopwords from all texts.\\n\")\n",
    "\n",
    "    # # Remove non-unique tweets\n",
    "    # X_unique, Y_unique, duplicated_tweets = remove_duplicates(X_no_stopwords, Y)\n",
    "    \n",
    "    return X_no_stopwords\n",
    "    # return X_unique, Y_unique, duplicated_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other cleaning steps that can be explored:\n",
    "- Handle non-unique tweets (PENDING! CRUCIAL.)\n",
    "- Consider preserving certain punctuations?\n",
    "- Spelling Corrections\n",
    "- Handling Slangs and Abbreviations\n",
    "- Stemming? (reduce words to their root word)\n",
    "- Handling emojis and special characters (found no emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with URLs: 5016\n",
      "Removed URLs. New count of text with URLs: 0\n",
      "\n",
      "Text with usernames: 3950\n",
      "Removed usernames. New count of text with usernames: 0\n",
      "\n",
      "Text with numeric words: 1260\n",
      "Removed numeric words. New count of text with numeric words: 0\n",
      "\n",
      "Text with HTML tags: 0\n",
      "Removed HTML tags. New count of text with HTML tags: 0\n",
      "\n",
      "Text with newlines: 0\n",
      "Removed newlines. New count of text with newlines: 0\n",
      "\n",
      "Converted all texts to lowercase.\n",
      "\n",
      "Identified 3016 contractions. Attempting to expand some of them.\n",
      "Expanded contractions. Remaining contractions: 1026 (Some are possessive.)\n",
      "\n",
      "Text with punctuations: 16779\n",
      "Removed punctuations. New count of text with punctuations: 0\n",
      "\n",
      "Removed stopwords from all texts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_X = cleaning_pipeline(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18464\n",
      "18464\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to cleaned_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_file(cleaned_X, Y, CLEANED_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Dataset from Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from cleaned_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(full_data_path=CLEANED_DATASET_PATH,\n",
    "                  from_scratch=False,\n",
    "                  split_sizes=[10000, 4232, 4232])\n",
    "dataset.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inaasahan vice president jejomar binay taong', 'mar roxas tang ina tuwid daan daw eh sya nga di straight', 'salamat sawang suporta taga makati pagbabalik binay makati onlybinayinmakatisankapa', 'putangina mo binay takbo', 'binay selective amnesia forgetting past six years spent preparing president pilipinasdebates2016']\n",
      "[0, 1, 0, 1, 0]\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_train = dataset.get_features(split_type=\"train\")\n",
    "Y_train = dataset.get_labels(split_type=\"train\")\n",
    "\n",
    "print(X_train[:5])\n",
    "print(Y_train[:5])\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['escudero denies betraying poe meeting binay karekare sinampalukan topic balimbing', 'hndi yung one chance saka kay binay fb haist', 'mar roxas addressing crowd gathered pasay citys ulat barangay', 'perfect makaharap duterte binay makatikim mura', 'onlybinaypriority4ps wag nating hayaan maloko pulitikong yan kay binay']\n",
      "[0, 1, 0, 0, 0]\n",
      "4232\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "X_val = dataset.get_features(split_type=\"val\")\n",
    "Y_val = dataset.get_labels(split_type=\"val\")\n",
    "\n",
    "print(X_val[:5])\n",
    "print(Y_val[:5])\n",
    "print(len(X_val))\n",
    "print(len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unshaded votes votes mayor duterte goes mar roxas according reports ballot tests ayawsadilaw', 'nomorechance', 'well good choices like duterte poe still undecided think', 'nognog pandak laki hirap corrupt yan si binay', 'exbinay aide turns tables mercado']\n",
      "[1, 1, 0, 1, 0]\n",
      "4232\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "X_test = dataset.get_features(split_type=\"test\")\n",
    "Y_test = dataset.get_labels(split_type=\"test\")\n",
    "\n",
    "print(X_test[:5])\n",
    "print(Y_test[:5])\n",
    "print(len(X_test))\n",
    "print(len(Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
