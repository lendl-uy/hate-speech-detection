{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from dataset import Dataset\n",
    "from collections import defaultdict\n",
    "from dataset_constants import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run when dataset_constants has been updated\n",
    "\n",
    "# import importlib\n",
    "# import dataset_constants\n",
    "# importlib.reload(dataset_constants) # reload to update changes to the file\n",
    "# from dataset_constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run when dataset.py has been updated\n",
    "\n",
    "# import importlib\n",
    "# import dataset\n",
    "# importlib.reload(dataset) # reload to update changes to the file\n",
    "# from dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 2022 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install datasets lib\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e166100369c94770ab7666c936ad6446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8c31479b534256ac085ffe30ff4a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112b15607b46403ba19e85e7656d85d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/315k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61835ac5b2c2467a82a72c39e3d0284c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/306k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b37a950ac94cc89e038bf56c1bcb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/21773 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd01fd65bdb24d88840085f88fad7689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856ef4d6574941bdb6ee6aa087ab0b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mapsoriano/2016_2022_hate_speech_filipino\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 21773\n",
      "Validation dataset size: 2800\n",
      "Test dataset size: 2810\n"
     ]
    }
   ],
   "source": [
    "# Access the train, validation, and test splits\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Print the size of each split to verify\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dataset['text']\n",
    "Y_train = train_dataset['label']\n",
    "\n",
    "X_val = validation_dataset['text']\n",
    "Y_val = validation_dataset['label']\n",
    "\n",
    "X_test = test_dataset['text']\n",
    "Y_test = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train + X_val + X_test\n",
    "Y = Y_train + Y_val + Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to uncleaned_2022dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "file_path = 'uncleaned_2022dataset.pkl'\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump((X, Y), f)\n",
    "    print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from uncleaned_2022dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "uncleaned_2022dataset = Dataset(full_data_path= file_path,\n",
    "    from_scratch=False,\n",
    "    split_sizes = [21773,2800,2810])\n",
    "uncleaned_2022dataset.build()\n",
    "\n",
    "X = uncleaned_2022dataset.get_features()\n",
    "Y = uncleaned_2022dataset.get_labels()\n",
    "X_train = uncleaned_2022dataset.get_features(split_type=\"train\")\n",
    "Y_train = uncleaned_2022dataset.get_labels(split_type=\"train\")\n",
    "X_val = uncleaned_2022dataset.get_features(split_type=\"val\")\n",
    "Y_val = uncleaned_2022dataset.get_labels(split_type=\"val\")\n",
    "X_test = uncleaned_2022dataset.get_features(split_type=\"test\")\n",
    "Y_test = uncleaned_2022dataset.get_labels(split_type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(train_path=TRAIN_DATASET_PATH,\n",
    "                  val_path=VALIDATION_DATASET_PATH, \n",
    "                  test_path=TEST_DATASET_PATH)\n",
    "dataset.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GASTOS NI VP BINAY SA POLITICAL ADS HALOS P7-M NA\\r\\rInaasahan na ni Vice President Jejomar Binay na may mga taong... https://t.co/SDytgbWiLh', 'Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA NGA DI STRAIGHT', 'Salamat sa walang sawang suporta ng mga taga makati! Ang Pagbabalik Binay In Makati #OnlyBinayInMakatiSanKaPa https://t.co/iwAOdtZPRE', '@rapplerdotcom putangina mo binay TAKBO PA', 'Binay with selective amnesia, forgetting about the past six years he spent preparing to be president.  #PiliPinasDebates2016']\n",
      "[0, 1, 0, 1, 0]\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_train = dataset.get_features(split_type=\"train\")\n",
    "Y_train = dataset.get_labels(split_type=\"train\")\n",
    "\n",
    "print(X_train[:5])\n",
    "print(Y_train[:5])\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Escudero denies betraying Poe after meeting with Binay |https://t.co/sKlXTIhHJa - Kare-kare at sinampalukan ang topic. Walang balimbing?', 'Hndi ko makita yung sa one more chance saka kay binay sa fb. Haist.', \"Mar Roxas is now addressing the crowd gathered at Pasay City's Ulat sa Barangay 2016  https://t.co/VruZyJ2e2H\", '@ImYourBaeMax perfect! Para makaharap ni Duterte ang mga Binay at makatikim ng mura #^%* i#*', '#OnlyBinayPriority4Ps Wag nating hayaan na maloko tayo ng mga pulitikong yan. Kay Binay na tayo']\n",
      "[0, 1, 0, 0, 0]\n",
      "4232\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "X_val = dataset.get_features(split_type=\"val\")\n",
    "Y_val = dataset.get_labels(split_type=\"val\")\n",
    "\n",
    "print(X_val[:5])\n",
    "print(Y_val[:5])\n",
    "print(len(X_val))\n",
    "print(len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unshaded votes and votes for Mayor Duterte goes to Mar Roxas according to some reports of ballot tests.  #AyawSaDILAW', 'Na-Binay ??????\\r#NoMoreChance https://t.co/msaaUGv0bS', \"@itsmanj well there's other good choices like Duterte or Poe. But both of them are still undecided, I think? :( :(\", 'Nognog. Pandak. Laki sa hirap. Pero corrupt. Yan si Binay!!!', 'Ex-Binay aide turns tables on Mercado | https://t.co/nyySAo54rL']\n",
      "[1, 1, 0, 1, 0]\n",
      "4232\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "X_test = dataset.get_features(split_type=\"test\")\n",
    "Y_test = dataset.get_labels(split_type=\"test\")\n",
    "\n",
    "print(X_test[:5])\n",
    "print(Y_test[:5])\n",
    "print(len(X_test))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18464\n",
      "18464\n",
      "['GASTOS NI VP BINAY SA POLITICAL ADS HALOS P7-M NA\\r\\rInaasahan na ni Vice President Jejomar Binay na may mga taong... https://t.co/SDytgbWiLh', 'Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA NGA DI STRAIGHT', 'Salamat sa walang sawang suporta ng mga taga makati! Ang Pagbabalik Binay In Makati #OnlyBinayInMakatiSanKaPa https://t.co/iwAOdtZPRE', '@rapplerdotcom putangina mo binay TAKBO PA', 'Binay with selective amnesia, forgetting about the past six years he spent preparing to be president.  #PiliPinasDebates2016']\n",
      "[0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "X = dataset.get_features()\n",
    "Y = dataset.get_labels()\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "print(X[:5])\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_texts(texts, pattern_type):\n",
    "    # Define the regex pattern\n",
    "    if pattern_type == 'url':\n",
    "        pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    elif pattern_type == 'username':\n",
    "        # Define the patterns\n",
    "        username_pattern = r'@\\S+'  # Matches usernames starting with @\n",
    "        placeholder_pattern = r'\\[USERNAME\\]'  # Matches [USERNAME]\n",
    "\n",
    "        # Combine the patterns using | (OR)\n",
    "        pattern = f'({username_pattern}|{placeholder_pattern})'\n",
    "    elif pattern_type == 'numeric':\n",
    "        pattern = r'\\b\\d+\\b'\n",
    "    elif pattern_type == 'html_tags':\n",
    "        pattern = r'<.*?>+'\n",
    "    elif pattern_type == 'newlines':\n",
    "        pattern = r'[\\r\\n]'\n",
    "    elif pattern_type == 'punctuation':\n",
    "        pattern = r'[’—‘`%s]' % re.escape(string.punctuation)\n",
    "    elif pattern_type == 'rt':\n",
    "        pattern = r'\\b(rt|RT)\\b'\n",
    "    elif pattern_type == 'possessive':\n",
    "        pattern = r\"('|’)s\\b\"\n",
    "    elif pattern_type == 'haha':\n",
    "        pattern = r'haha'\n",
    "    elif pattern_type == 'hashtag':\n",
    "        pattern = r'#\\w+'\n",
    "    elif pattern_type == 'emoji':\n",
    "        pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               \"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each text and check for the presence of the pattern\n",
    "    for text in texts:\n",
    "        if re.search(pattern, text):\n",
    "            count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(texts, pattern_type):\n",
    "    # Define the regex pattern\n",
    "    if pattern_type == 'url':\n",
    "        pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    elif pattern_type == 'username':\n",
    "        # Define the patterns\n",
    "        username_pattern = r'@\\S+'  # Matches usernames starting with @\n",
    "        placeholder_pattern = r'\\[USERNAME\\]'  # Matches [USERNAME]\n",
    "\n",
    "        # Combine the patterns using | (OR)\n",
    "        pattern = f'({username_pattern}|{placeholder_pattern})'\n",
    "        return [re.sub(pattern, ' ', text) for text in texts]\n",
    "    \n",
    "    elif pattern_type == 'numeric':\n",
    "        pattern = r'\\b\\d+\\b'\n",
    "    elif pattern_type == 'html_tags':\n",
    "        pattern = r'<.*?>+'\n",
    "    elif pattern_type == 'newlines':\n",
    "        pattern = r'[\\r\\n]'\n",
    "        return [re.sub(pattern, ' ', text) for text in texts]\n",
    "    elif pattern_type == 'punctuation':\n",
    "        pattern = r'[’—‘`%s]' % re.escape(string.punctuation)\n",
    "        return [re.sub(pattern, ' ', text) for text in texts]\n",
    "    elif pattern_type == 'rt':\n",
    "        pattern = r'\\b(rt|RT)\\b'\n",
    "    elif pattern_type == 'possessive':\n",
    "        pattern = r\"('|’)s\\b\"\n",
    "    elif pattern_type == 'hashtag':\n",
    "        pattern = r'#\\w+'\n",
    "    elif pattern_type == 'haha':\n",
    "        pattern = r'haha'\n",
    "        return [re.sub(pattern, 'haha', text) for text in texts]\n",
    "\n",
    "    return [re.sub(pattern, '', text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_contractions = {\n",
    "    \"gov't\": \"government\",\n",
    "    \"s'ya\": \"siya\",\n",
    "    \"sya\": \"siya\",\n",
    "    \"sa'yo\": \"sa iyo\",\n",
    "    \"ika'y\": \"ikaw ay\",\n",
    "    \"everybody's\": \"everybody is\",\n",
    "    \"mo'ko\": \"mo ako\",\n",
    "    \"ba't\": \"bakit\",\n",
    "    \"sila'y\": \"sila ay\",\n",
    "    \"aba'y\": \"aba ay\",\n",
    "    \"ito'y\": \"ito ay\",\n",
    "    \"mgm't\": \"management\",\n",
    "    \"shut'up\": \"shut up\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"umano'y\": \"umano ay\",\n",
    "    \"kaya't\": \"kaya at\",\n",
    "    \"n'ya\": \"niya\",\n",
    "    \"le'me\": \"let me\",\n",
    "    \"c'mon\": \"common\",\n",
    "    \"isa't\": \"isa at\",\n",
    "    \"ako'y\": \"ako ay\",\n",
    "    \"toyo't\": \"toyo at\",\n",
    "    \"na'to\": \"na ito\",\n",
    "    \"n'yo\": \"niyo\"\n",
    "}\n",
    "\n",
    "def expand_all_contractions(text, custom_dict):\n",
    "    # First, expand using the default package\n",
    "    expanded_text = contractions.fix(text)\n",
    "    # Now apply custom contractions\n",
    "    for key, value in custom_dict.items():\n",
    "        expanded_text = expanded_text.replace(key, value)\n",
    "    return expanded_text\n",
    "\n",
    "def count_contractions(texts):\n",
    "    # Regular expression to match contractions\n",
    "    contraction_pattern = r\"\\b\\w+['’]\\w+\\b\"\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each text and count contractions\n",
    "    for text in texts:\n",
    "        # Find all instances of the pattern\n",
    "        contracted_words = re.findall(contraction_pattern, text)\n",
    "        count += len(contracted_words)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "    r'\\btang\\s+ina\\b': 'tangina',\n",
    "    r'\\bwtf\\b': 'what the fuck',\n",
    "    r'\\bt@ng@\\b': 'tanga',\n",
    "    r'\\bt@ng!n@\\b': 'tangina',\n",
    "    r'p\\*\\*\\*\\*\\* i\\*\\*': 'tangina',  \n",
    "    r'\\bputangina\\b': 'tangina',\n",
    "    r'\\bpota\\b': 'puta'\n",
    "}\n",
    "\n",
    "def replace_custom_words(text, replacements):\n",
    "    for pattern, replacement in replacements.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Function to count occurrences of custom words\n",
    "def contains_any_pattern(text, patterns):\n",
    "    # Check if any of the patterns exist in the text\n",
    "    for pattern in patterns.keys():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def count_tweets_with_patterns(tweets, patterns):\n",
    "    count = 0\n",
    "    for tweet in tweets:\n",
    "        if contains_any_pattern(tweet, patterns):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read stopwords from file and save into a list\n",
    "def read_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords_list = [word.strip() for word in file.readlines()]\n",
    "    return stopwords_list\n",
    "\n",
    "# Read English stopwords\n",
    "english_stopwords = read_stopwords(STOPWORDS_ENGLISH_PATH)\n",
    "\n",
    "# Read Tagalog stopwords\n",
    "tagalog_stopwords = read_stopwords(STOPWORDS_TAGALOG_PATH)\n",
    "\n",
    "# Add custom stopwords - determined based on most common words (and contractions of existing stopwords)\n",
    "custom_stopwords = ['si','kay','lang','yung','wag','ba','yan','iyan','kayo','pag','naman','mo','niyo','nung','kang','tong','nalang']\n",
    "\n",
    "# Combine stopwords\n",
    "combined_stopwords = set(english_stopwords + tagalog_stopwords + custom_stopwords)\n",
    "\n",
    "# Specify words you don't want to consider as stopwords\n",
    "exclude_words = {'not','di','hindi','wala'}\n",
    "\n",
    "# Update the combined stopwords set to exclude certain words\n",
    "combined_stopwords = combined_stopwords - exclude_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Filter out the stopwords\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    # Join the filtered words back into a single string\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(X, y):\n",
    "    # Create a dictionary to store indices of non-unique tweets\n",
    "    non_unique_indices = defaultdict(list)\n",
    "\n",
    "    # Iterate over the feature data X and store the indices of non-unique tweets\n",
    "    for i, tweet in enumerate(X):\n",
    "        non_unique_indices[tweet].append(i)\n",
    "\n",
    "    # Identify the indices of duplicates\n",
    "    non_unique_tweets_indices = [indices[0] for indices in non_unique_indices.values() if len(indices) > 1] # get first occurrence of nonunique tweets\n",
    "    duplicate_indices = [indices[:-1] for indices in non_unique_indices.values() if len(indices) > 1] \n",
    "        # we do indices[:-1] so the LAST occurrence will not be removed (we take the last occurrence instead of the first so we remove as little as possible from the validation and test sets)\n",
    "\n",
    "    # Flatten the list of duplicate indices\n",
    "    duplicate_indices = [idx for sublist in duplicate_indices for idx in sublist]\n",
    "\n",
    "    # Remove duplicates from X and y\n",
    "    X_unique = [X[i] for i in range(len(X)) if i not in duplicate_indices]\n",
    "    y_unique = [y[i] for i in range(len(y)) if i not in duplicate_indices]\n",
    "\n",
    "    # Get non unique tweets\n",
    "    non_unique_tweets = [X[i] for i in non_unique_tweets_indices]\n",
    "\n",
    "    # Now, count how many removed items belong to each set\n",
    "    removed_train_count = sum(1 for idx in duplicate_indices if idx < split_sizes[0])\n",
    "    removed_validation_count = sum(1 for idx in duplicate_indices if split_sizes[0] <= idx < split_sizes[0] + split_sizes[1])\n",
    "    removed_test_count = sum(1 for idx in duplicate_indices if split_sizes[0] + split_sizes[1] <= idx)\n",
    "    total_removed = removed_train_count + removed_validation_count + removed_test_count\n",
    "\n",
    "    print(f\"Removed {total_removed} non-unique tweets.\")\n",
    "    print(\"Removed from train set:\", removed_train_count)\n",
    "    print(\"Removed from validation set:\", removed_validation_count)\n",
    "    print(\"Removed from test set:\", removed_test_count)\n",
    "\n",
    "    return X_unique, y_unique, non_unique_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_and_count_candidate_names(tweets):\n",
    "    # List of words to remove\n",
    "    words_to_remove = {\n",
    "        \"jejomar\", \"binay\", \"mar\", \"roxas\", \"rodrigo\", \n",
    "        \"duterte\", \"grace\", \"poe\", \"miriam\", \"defensor\", \"santiago\"\n",
    "    }\n",
    "    \n",
    "    # Initialize a counter for the occurrences of these words\n",
    "    count = 0\n",
    "    \n",
    "    # Define a regex pattern to match any of the words in the list\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(words_to_remove) + r')\\b', re.IGNORECASE)\n",
    "    \n",
    "    # Process each tweet\n",
    "    cleaned_tweets = []\n",
    "    for tweet in tweets:\n",
    "        # Count occurrences of words in the tweet\n",
    "        found_words = pattern.findall(tweet)\n",
    "        count += len(found_words)\n",
    "        \n",
    "        # Remove all instances of the specified words\n",
    "        cleaned_tweet = pattern.sub('', tweet)\n",
    "        cleaned_tweet = re.sub(r'\\s+', ' ', cleaned_tweet).strip()  # Normalize spaces\n",
    "        cleaned_tweets.append(cleaned_tweet)\n",
    "    \n",
    "    return cleaned_tweets, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_pipeline(X, Y):\n",
    "    # Remove newlines\n",
    "    print(f\"Text with newlines: {count_texts(X, 'newlines')}\")\n",
    "    X_newlines_removed = clean_texts(X, 'newlines')\n",
    "    print(f\"Removed newlines. New count of text with newlines: {count_texts(X_newlines_removed, 'newlines')}\\n\")\n",
    "\n",
    "    # Remove URLs\n",
    "    print(f\"Text with URLs: {count_texts(X_newlines_removed, 'url')}\")\n",
    "    X_url_removed = clean_texts(X_newlines_removed, 'url')\n",
    "    print(f\"Removed URLs. New count of text with URLs: {count_texts(X_url_removed, 'url')}\\n\")\n",
    "\n",
    "    # Remove usernames\n",
    "    print(f\"Text with usernames: {count_texts(X_url_removed, 'username')}\")\n",
    "    X_username_removed = clean_texts(X_url_removed, 'username')\n",
    "    print(f\"Removed usernames. New count of text with usernames: {count_texts(X_username_removed, 'username')}\\n\")\n",
    "\n",
    "    # Remove words that are completely numbers\n",
    "    print(f\"Text with numeric words: {count_texts(X_username_removed, 'numeric')}\")\n",
    "    X_numeric_removed = clean_texts(X_username_removed, 'numeric')\n",
    "    print(f\"Removed numeric words. New count of text with numeric words: {count_texts(X_numeric_removed, 'numeric')}\\n\")\n",
    "\n",
    "    # Remove HTML tags\n",
    "    print(f\"Text with HTML tags: {count_texts(X_numeric_removed, 'html_tags')}\")\n",
    "    X_html_removed = clean_texts(X_numeric_removed, 'html_tags')\n",
    "    print(f\"Removed HTML tags. New count of text with HTML tags: {count_texts(X_html_removed, 'html_tags')}\\n\")\n",
    "\n",
    "    # Lowercase all texts\n",
    "    X_lowercased = [text.lower() for text in X_html_removed]\n",
    "    print(f\"Converted all texts to lowercase.\\n\")\n",
    "\n",
    "    # Remove \"rt\" from tweets\n",
    "    print(f\"Text with 'RT': {count_texts(X_lowercased, 'rt')}\")\n",
    "    X_rt_removed = clean_texts(X_lowercased, 'rt')\n",
    "    print(f\"Removed 'RT'. New count of text with 'RT': {count_texts(X_rt_removed, 'rt')}\\n\")\n",
    "\n",
    "    # Shorten all variations of \"haha\"\n",
    "    print(f\"Text with variations of 'haha': {count_texts(X_rt_removed, 'haha')}\")\n",
    "    X_haha_removed = clean_texts(X_rt_removed, 'haha')\n",
    "    print(f\"Shortened all variations of 'haha'. New count of text with 'haha': {count_texts(X_haha_removed, 'haha')}\\n\")\n",
    "\n",
    "    # Remove hashtags\n",
    "    print(f\"Text with hashtags: {count_texts(X_haha_removed, 'hashtag')}\")\n",
    "    X_hashtags_removed = clean_texts(X_haha_removed, 'hashtag')\n",
    "    print(f\"Removed hashtags. New count of text with hashtags: {count_texts(X_hashtags_removed, 'hashtag')}\\n\")\n",
    "\n",
    "    # Expand contractions\n",
    "    print(f\"Identified {count_contractions(X_hashtags_removed)} contractions. Attempting to expand some of them.\")\n",
    "    X_expanded = [expand_all_contractions(text, custom_contractions) for text in X_hashtags_removed]\n",
    "    print(f\"Expanded contractions. Remaining contractions: {count_contractions(X_expanded)} (Some are possessive.)\\n\")\n",
    "\n",
    "    # Remove possessives\n",
    "    print(f\"Text with possessives: {count_texts(X_expanded, 'possessive')}\")\n",
    "    X_no_possessives = clean_texts(X_expanded, 'possessive')\n",
    "    print(f\"Removed possessives. New count of text with possessives: {count_texts(X_no_possessives, 'possessive')}\\n\")\n",
    "\n",
    "    # Remove punctuations - CAN CONSIDER NOT REMOVING\n",
    "    print(f\"Text with punctuations: {count_texts(X_no_possessives, 'punctuation')}\")\n",
    "    X_punctuation_removed = clean_texts(X_no_possessives, 'punctuation')\n",
    "    print(f\"Removed punctuations. New count of text with punctuations: {count_texts(X_punctuation_removed, 'punctuation')}\\n\")\n",
    "\n",
    "   # Replace custom words and phrases\n",
    "    print(f\"Count of tweets containing words to be replaced: {count_tweets_with_patterns(X_punctuation_removed, replacements)}\")\n",
    "    X_custom_replaced = [replace_custom_words(text, replacements) for text in X_punctuation_removed]\n",
    "    print(f\"Count after custom replacements: {count_tweets_with_patterns(X_custom_replaced, replacements)}\\n\")\n",
    "\n",
    "    # Remove stop words - both English and Filipino\n",
    "    X_no_stopwords = [remove_stopwords(text, combined_stopwords) for text in X_custom_replaced]\n",
    "    print(\"Removed stopwords from all texts.\\n\")\n",
    "    # return X_no_stopwords, Y\n",
    "\n",
    "    # Remove non-unique tweets\n",
    "    X_unique, Y_unique, non_unique_tweets = remove_duplicates(X_no_stopwords, Y)\n",
    "    return X_unique, Y_unique\n",
    "\n",
    "    # # Remove candidate names\n",
    "    # cleaned_tweets, count = remove_and_count_candidate_names(X_unique)\n",
    "    # print(f\"\\nRemoved candidate names. Count of removed words: {count}\")\n",
    "    # return cleaned_tweets, Y_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other cleaning steps that can be explored:\n",
    "- Consider preserving certain punctuations?\n",
    "- Spelling Corrections\n",
    "- Handling Slangs and Abbreviations\n",
    "- Stemming? (reduce words to their root word)\n",
    "- Handling emojis and special characters (found no emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with newlines: 11\n",
      "Removed newlines. New count of text with newlines: 0\n",
      "\n",
      "Text with URLs: 32\n",
      "Removed URLs. New count of text with URLs: 0\n",
      "\n",
      "Text with usernames: 8661\n",
      "Removed usernames. New count of text with usernames: 0\n",
      "\n",
      "Text with numeric words: 1216\n",
      "Removed numeric words. New count of text with numeric words: 0\n",
      "\n",
      "Text with HTML tags: 0\n",
      "Removed HTML tags. New count of text with HTML tags: 0\n",
      "\n",
      "Converted all texts to lowercase.\n",
      "\n",
      "Text with 'RT': 696\n",
      "Removed 'RT'. New count of text with 'RT': 0\n",
      "\n",
      "Text with variations of 'haha': 1949\n",
      "Shortened all variations of 'haha'. New count of text with 'haha': 1949\n",
      "\n",
      "Text with hashtags: 7\n",
      "Removed hashtags. New count of text with hashtags: 0\n",
      "\n",
      "Identified 2803 contractions. Attempting to expand some of them.\n",
      "Expanded contractions. Remaining contractions: 999 (Some are possessive.)\n",
      "\n",
      "Text with possessives: 761\n",
      "Removed possessives. New count of text with possessives: 0\n",
      "\n",
      "Text with punctuations: 18774\n",
      "Removed punctuations. New count of text with punctuations: 0\n",
      "\n",
      "Count of tweets containing words to be replaced: 315\n",
      "Count after custom replacements: 0\n",
      "\n",
      "Removed stopwords from all texts.\n",
      "\n",
      "Removed 965 non-unique tweets.\n",
      "Removed from train set: 491\n",
      "Removed from validation set: 156\n",
      "Removed from test set: 318\n"
     ]
    }
   ],
   "source": [
    "cleaned_X, cleaned_Y = cleaning_pipeline(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26418\n",
      "26418\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_X))\n",
    "print(len(cleaned_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to cleaned_2022dataset_v1.pkl\n"
     ]
    }
   ],
   "source": [
    "file_path = 'cleaned_2022dataset_v1.pkl'\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump((cleaned_X, cleaned_Y), f)\n",
    "    print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_file(cleaned_X, cleaned_Y, 'cleaned_dataset_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Dataset from Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New train-val-test split after data cleaning: \n",
    "- Train: 10000-1007 = 8993\n",
    "- Validation: 4232-275 = 3957\n",
    "- Test: 4232-133 = 4099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from cleaned_dataset_v1.pkl\n"
     ]
    }
   ],
   "source": [
    "cleaned_dataset = Dataset(full_data_path= 'cleaned_dataset_v1.pkl',\n",
    "                  from_scratch=False,\n",
    "                  split_sizes = [8993,3957,4099])\n",
    "cleaned_dataset.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gastos vp binay political ads halos p7 inaasahan vice president jejomar binay taong', 'mar roxas tangina tuwid daan daw eh nga di straight', 'salamat sawang suporta taga makati pagbabalik binay makati', 'tangina binay takbo', 'binay selective amnesia forgetting past six years spent preparing president']\n",
      "[0, 1, 0, 1, 0]\n",
      "17049\n",
      "17049\n"
     ]
    }
   ],
   "source": [
    "X = cleaned_dataset.get_features()\n",
    "Y = cleaned_dataset.get_labels()\n",
    "\n",
    "print(X[:5])\n",
    "print(Y[:5])\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gastos vp binay political ads halos p7 inaasahan vice president jejomar binay taong', 'mar roxas tangina tuwid daan daw eh nga di straight', 'salamat sawang suporta taga makati pagbabalik binay makati', 'tangina binay takbo', 'binay selective amnesia forgetting past six years spent preparing president']\n",
      "[0, 1, 0, 1, 0]\n",
      "8993\n",
      "8993\n"
     ]
    }
   ],
   "source": [
    "X_train = cleaned_dataset.get_features(split_type=\"train\")\n",
    "Y_train = cleaned_dataset.get_labels(split_type=\"train\")\n",
    "\n",
    "print(X_train[:5])\n",
    "print(Y_train[:5])\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hndi one chance saka binay fb haist', 'mar roxas addressing crowd gathered pasay city ulat barangay', 'perfect makaharap duterte binay makatikim mura', 'nating hayaan maloko pulitikong binay', 'regards advertistment binay haha']\n",
      "[1, 0, 0, 0, 0]\n",
      "3957\n",
      "3957\n"
     ]
    }
   ],
   "source": [
    "X_val = cleaned_dataset.get_features(split_type=\"val\")\n",
    "Y_val = cleaned_dataset.get_labels(split_type=\"val\")\n",
    "\n",
    "print(X_val[:5])\n",
    "print(Y_val[:5])\n",
    "print(len(X_val))\n",
    "print(len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unshaded votes votes mayor duterte goes mar roxas according reports ballot tests', 'well good choices like duterte poe still undecided think', 'nognog pandak laki hirap corrupt binay', 'ex binay aide turns tables mercado', 'bayan muna everydayiloveyou blogcon tomiho momentwithaimi andiloveyouso kenzo abby binay pht']\n",
      "[1, 0, 1, 0, 0]\n",
      "4099\n",
      "4099\n"
     ]
    }
   ],
   "source": [
    "X_test = cleaned_dataset.get_features(split_type=\"test\")\n",
    "Y_test = cleaned_dataset.get_labels(split_type=\"test\")\n",
    "\n",
    "print(X_test[:5])\n",
    "print(Y_test[:5])\n",
    "print(len(X_test))\n",
    "print(len(Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
