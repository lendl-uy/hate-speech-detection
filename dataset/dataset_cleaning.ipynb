{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from dataset import Dataset\n",
    "from collections import defaultdict\n",
    "from dataset_constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run when dataset_constants has been updated\n",
    "\n",
    "# import importlib\n",
    "# import dataset_constants\n",
    "# importlib.reload(dataset_constants) # reload to update changes to the file\n",
    "# from dataset_constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run when dataset.py has been updated\n",
    "\n",
    "# import importlib\n",
    "# import dataset\n",
    "# importlib.reload(dataset) # reload to update changes to the file\n",
    "# from dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(train_path=TRAIN_DATASET_PATH,\n",
    "                  val_path=VALIDATION_DATASET_PATH, \n",
    "                  test_path=TEST_DATASET_PATH)\n",
    "dataset.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GASTOS NI VP BINAY SA POLITICAL ADS HALOS P7-M NA\\r\\rInaasahan na ni Vice President Jejomar Binay na may mga taong... https://t.co/SDytgbWiLh', 'Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA NGA DI STRAIGHT', 'Salamat sa walang sawang suporta ng mga taga makati! Ang Pagbabalik Binay In Makati #OnlyBinayInMakatiSanKaPa https://t.co/iwAOdtZPRE', '@rapplerdotcom putangina mo binay TAKBO PA', 'Binay with selective amnesia, forgetting about the past six years he spent preparing to be president.  #PiliPinasDebates2016']\n",
      "[0, 1, 0, 1, 0]\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_train = dataset.get_features(split_type=\"train\")\n",
    "Y_train = dataset.get_labels(split_type=\"train\")\n",
    "\n",
    "print(X_train[:5])\n",
    "print(Y_train[:5])\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Escudero denies betraying Poe after meeting with Binay |https://t.co/sKlXTIhHJa - Kare-kare at sinampalukan ang topic. Walang balimbing?', 'Hndi ko makita yung sa one more chance saka kay binay sa fb. Haist.', \"Mar Roxas is now addressing the crowd gathered at Pasay City's Ulat sa Barangay 2016  https://t.co/VruZyJ2e2H\", '@ImYourBaeMax perfect! Para makaharap ni Duterte ang mga Binay at makatikim ng mura #^%* i#*', '#OnlyBinayPriority4Ps Wag nating hayaan na maloko tayo ng mga pulitikong yan. Kay Binay na tayo']\n",
      "[0, 1, 0, 0, 0]\n",
      "4232\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "X_val = dataset.get_features(split_type=\"val\")\n",
    "Y_val = dataset.get_labels(split_type=\"val\")\n",
    "\n",
    "print(X_val[:5])\n",
    "print(Y_val[:5])\n",
    "print(len(X_val))\n",
    "print(len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unshaded votes and votes for Mayor Duterte goes to Mar Roxas according to some reports of ballot tests.  #AyawSaDILAW', 'Na-Binay ??????\\r#NoMoreChance https://t.co/msaaUGv0bS', \"@itsmanj well there's other good choices like Duterte or Poe. But both of them are still undecided, I think? :( :(\", 'Nognog. Pandak. Laki sa hirap. Pero corrupt. Yan si Binay!!!', 'Ex-Binay aide turns tables on Mercado | https://t.co/nyySAo54rL']\n",
      "[1, 1, 0, 1, 0]\n",
      "4232\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "X_test = dataset.get_features(split_type=\"test\")\n",
    "Y_test = dataset.get_labels(split_type=\"test\")\n",
    "\n",
    "print(X_test[:5])\n",
    "print(Y_test[:5])\n",
    "print(len(X_test))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18464\n",
      "18464\n",
      "['GASTOS NI VP BINAY SA POLITICAL ADS HALOS P7-M NA\\r\\rInaasahan na ni Vice President Jejomar Binay na may mga taong... https://t.co/SDytgbWiLh', 'Mar Roxas TANG INA TUWID NA DAAN DAW .. EH SYA NGA DI STRAIGHT', 'Salamat sa walang sawang suporta ng mga taga makati! Ang Pagbabalik Binay In Makati #OnlyBinayInMakatiSanKaPa https://t.co/iwAOdtZPRE', '@rapplerdotcom putangina mo binay TAKBO PA', 'Binay with selective amnesia, forgetting about the past six years he spent preparing to be president.  #PiliPinasDebates2016']\n",
      "[0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "X = dataset.get_features()\n",
    "Y = dataset.get_labels()\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "print(X[:5])\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_texts(texts, pattern_type):\n",
    "    # Define the regex pattern\n",
    "    if pattern_type == 'url':\n",
    "        pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    elif pattern_type == 'username':\n",
    "        pattern = r'@\\S+'\n",
    "    elif pattern_type == 'numeric':\n",
    "        pattern = r'\\b\\d+\\b'\n",
    "    elif pattern_type == 'html_tags':\n",
    "        pattern = r'<.*?>+'\n",
    "    elif pattern_type == 'newlines':\n",
    "        pattern = r'\\n'\n",
    "    elif pattern_type == 'punctuation':\n",
    "        pattern = r'[’—‘`%s]' % re.escape(string.punctuation)\n",
    "    elif pattern_type == 'rt':\n",
    "        pattern = r'\\b(rt|RT)\\b'\n",
    "    elif pattern_type == 'possessive':\n",
    "        pattern = r\"('|’)s\\b\"\n",
    "    elif pattern_type == 'emoji':\n",
    "        pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               \"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each text and check for the presence of the pattern\n",
    "    for text in texts:\n",
    "        if re.search(pattern, text):\n",
    "            count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(texts, pattern_type):\n",
    "    # Define the regex pattern\n",
    "    if pattern_type == 'url':\n",
    "        pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    elif pattern_type == 'username':\n",
    "        pattern = r'@\\S+'\n",
    "    elif pattern_type == 'numeric':\n",
    "        pattern = r'\\b\\d+\\b'\n",
    "    elif pattern_type == 'html_tags':\n",
    "        pattern = r'<.*?>+'\n",
    "    elif pattern_type == 'newlines':\n",
    "        pattern = r'\\n'\n",
    "    elif pattern_type == 'punctuation':\n",
    "        pattern = r'[’—‘`%s]' % re.escape(string.punctuation)\n",
    "    elif pattern_type == 'rt':\n",
    "        pattern = r'\\b(rt|RT)\\b'\n",
    "    elif pattern_type == 'possessive':\n",
    "        pattern = r\"('|’)s\\b\"\n",
    "\n",
    "    return [re.sub(pattern, '', text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_contractions = {\n",
    "    \"gov't\": \"government\",\n",
    "    \"s'ya\": \"siya\",\n",
    "    \"sa'yo\": \"sa iyo\",\n",
    "    \"ika'y\": \"ikaw ay\",\n",
    "    \"everybody's\": \"everybody is\",\n",
    "    \"mo'ko\": \"mo ako\",\n",
    "    \"ba't\": \"bakit\",\n",
    "    \"sila'y\": \"sila ay\",\n",
    "    \"aba'y\": \"aba ay\",\n",
    "    \"ito'y\": \"ito ay\",\n",
    "    \"mgm't\": \"management\",\n",
    "    \"shut'up\": \"shut up\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"umano'y\": \"umano ay\",\n",
    "    \"kaya't\": \"kaya at\",\n",
    "    \"n'ya\": \"niya\",\n",
    "    \"le'me\": \"let me\",\n",
    "    \"c'mon\": \"common\",\n",
    "    \"isa't\": \"isa at\",\n",
    "    \"ako'y\": \"ako ay\",\n",
    "    \"toyo't\": \"toyo at\",\n",
    "    \"na'to\": \"na ito\",\n",
    "    \"n'yo\": \"niyo\"\n",
    "}\n",
    "\n",
    "def expand_all_contractions(text, custom_dict):\n",
    "    # First, expand using the default package\n",
    "    expanded_text = contractions.fix(text)\n",
    "    # Now apply custom contractions\n",
    "    for key, value in custom_dict.items():\n",
    "        expanded_text = expanded_text.replace(key, value)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_contractions(texts):\n",
    "    # Regular expression to match contractions\n",
    "    contraction_pattern = r\"\\b\\w+['’]\\w+\\b\"\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through each text and count contractions\n",
    "    for text in texts:\n",
    "        # Find all instances of the pattern\n",
    "        contracted_words = re.findall(contraction_pattern, text)\n",
    "        count += len(contracted_words)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read stopwords from file and save into a list\n",
    "def read_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords_list = [word.strip() for word in file.readlines()]\n",
    "    return stopwords_list\n",
    "\n",
    "# Read English stopwords\n",
    "english_stopwords = read_stopwords(STOPWORDS_ENGLISH_PATH)\n",
    "\n",
    "# Read Tagalog stopwords\n",
    "tagalog_stopwords = read_stopwords(STOPWORDS_TAGALOG_PATH)\n",
    "\n",
    "# Add custom stopwords - determined based on most common words (and contractions of existing stopwords)\n",
    "custom_stopwords = ['si','kay','lang','yung','wag','ba','di','yan','iyan','kayo','pag','naman']\n",
    "\n",
    "# Combine stopwords\n",
    "combined_stopwords = set(english_stopwords + tagalog_stopwords + custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Filter out the stopwords\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    # Join the filtered words back into a single string\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(X, y):\n",
    "    # Create a dictionary to store indices of non-unique tweets\n",
    "    non_unique_indices = defaultdict(list)\n",
    "\n",
    "    # Iterate over the feature data X and store the indices of non-unique tweets\n",
    "    for i, tweet in enumerate(X):\n",
    "        non_unique_indices[tweet].append(i)\n",
    "\n",
    "    # Identify the indices of duplicates\n",
    "    non_unique_tweets_indices = [indices[0] for indices in non_unique_indices.values() if len(indices) > 1] # get first occurrence of nonunique tweets\n",
    "    duplicate_indices = [indices[:-1] for indices in non_unique_indices.values() if len(indices) > 1] \n",
    "        # we do indices[:-1] so the LAST occurrence will not be removed (we take the last occurrence instead of the first so we remove as little as possible from the validation and test sets)\n",
    "\n",
    "    # Flatten the list of duplicate indices\n",
    "    duplicate_indices = [idx for sublist in duplicate_indices for idx in sublist]\n",
    "\n",
    "    # Remove duplicates from X and y\n",
    "    X_unique = [X[i] for i in range(len(X)) if i not in duplicate_indices]\n",
    "    y_unique = [y[i] for i in range(len(y)) if i not in duplicate_indices]\n",
    "\n",
    "    # Get non unique tweets\n",
    "    non_unique_tweets = [X[i] for i in non_unique_tweets_indices]\n",
    "\n",
    "    # Now, count how many removed items belong to each set\n",
    "    removed_train_count = sum(1 for idx in duplicate_indices if idx < split_sizes[0])\n",
    "    removed_validation_count = sum(1 for idx in duplicate_indices if split_sizes[0] <= idx < split_sizes[0] + split_sizes[1])\n",
    "    removed_test_count = sum(1 for idx in duplicate_indices if split_sizes[0] + split_sizes[1] <= idx)\n",
    "    total_removed = removed_train_count + removed_validation_count + removed_test_count\n",
    "\n",
    "    print(f\"Removed {total_removed} non-unique tweets.\")\n",
    "    print(\"Removed from train set:\", removed_train_count)\n",
    "    print(\"Removed from validation set:\", removed_validation_count)\n",
    "    print(\"Removed from test set:\", removed_test_count)\n",
    "\n",
    "    return X_unique, y_unique, non_unique_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_pipeline(X, Y):\n",
    "    # Remove URLs\n",
    "    print(f\"Text with URLs: {count_texts(X, 'url')}\")\n",
    "    X_url_removed = clean_texts(X, 'url')\n",
    "    print(f\"Removed URLs. New count of text with URLs: {count_texts(X_url_removed, 'url')}\\n\")\n",
    "\n",
    "    # Remove usernames\n",
    "    print(f\"Text with usernames: {count_texts(X_url_removed, 'username')}\")\n",
    "    X_username_removed = clean_texts(X_url_removed, 'username')\n",
    "    print(f\"Removed usernames. New count of text with usernames: {count_texts(X_username_removed, 'username')}\\n\")\n",
    "\n",
    "    # Remove words that are completely numbers\n",
    "    print(f\"Text with numeric words: {count_texts(X_username_removed, 'numeric')}\")\n",
    "    X_numeric_removed = clean_texts(X_username_removed, 'numeric')\n",
    "    print(f\"Removed numeric words. New count of text with numeric words: {count_texts(X_numeric_removed, 'numeric')}\\n\")\n",
    "\n",
    "    # Remove HTML tags\n",
    "    print(f\"Text with HTML tags: {count_texts(X_numeric_removed, 'html_tags')}\")\n",
    "    X_html_removed = clean_texts(X_numeric_removed, 'html_tags')\n",
    "    print(f\"Removed HTML tags. New count of text with HTML tags: {count_texts(X_html_removed, 'html_tags')}\\n\")\n",
    "\n",
    "    # Remove newlines\n",
    "    print(f\"Text with newlines: {count_texts(X_html_removed, 'newlines')}\")\n",
    "    X_newlines_removed = clean_texts(X_html_removed, 'newlines')\n",
    "    print(f\"Removed newlines. New count of text with newlines: {count_texts(X_newlines_removed, 'newlines')}\\n\")\n",
    "\n",
    "    # Lowercase all texts\n",
    "    X_lowercased = [text.lower() for text in X_newlines_removed]\n",
    "    print(f\"Converted all texts to lowercase.\\n\")\n",
    "\n",
    "    # Remove \"rt\" from tweets\n",
    "    print(f\"Text with 'RT': {count_texts(X_lowercased, 'rt')}\")\n",
    "    X_rt_removed = clean_texts(X_lowercased, 'rt')\n",
    "    print(f\"Removed 'RT'. New count of text with 'RT': {count_texts(X_rt_removed, 'rt')}\\n\")\n",
    "\n",
    "    # Expand contractions\n",
    "    print(f\"Identified {count_contractions(X_rt_removed)} contractions. Attempting to expand some of them.\")\n",
    "    X_expanded = [expand_all_contractions(text, custom_contractions) for text in X_rt_removed]\n",
    "    print(f\"Expanded contractions. Remaining contractions: {count_contractions(X_expanded)} (Some are possessive.)\\n\")\n",
    "\n",
    "    # Remove possessives\n",
    "    print(f\"Text with possessives: {count_texts(X_expanded, 'possessive')}\")\n",
    "    X_no_possessives = clean_texts(X_expanded, 'possessive')\n",
    "    print(f\"Removed possessives. New count of text with possessives: {count_texts(X_no_possessives, 'possessive')}\\n\")\n",
    "\n",
    "    # Remove punctuations - CAN CONSIDER NOT REMOVING\n",
    "    print(f\"Text with punctuations: {count_texts(X_no_possessives, 'punctuation')}\")\n",
    "    X_punctuation_removed = clean_texts(X_no_possessives, 'punctuation')\n",
    "    print(f\"Removed punctuations. New count of text with punctuations: {count_texts(X_punctuation_removed, 'punctuation')}\\n\")\n",
    "\n",
    "    # Remove stop words - both English and Filipino\n",
    "    X_no_stopwords = [remove_stopwords(text, combined_stopwords) for text in X_punctuation_removed]\n",
    "    print(\"Removed stopwords from all texts.\\n\")\n",
    "\n",
    "    # # Remove non-unique tweets\n",
    "    # X_unique, Y_unique, non_unique_tweets = remove_duplicates(X_no_stopwords, Y)\n",
    "    # return X_unique, Y_unique, non_unique_tweets\n",
    "\n",
    "    return X_no_stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other cleaning steps that can be explored:\n",
    "- Consider preserving certain punctuations?\n",
    "- Spelling Corrections\n",
    "- Handling Slangs and Abbreviations\n",
    "- Stemming? (reduce words to their root word)\n",
    "- Handling emojis and special characters (found no emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with URLs: 5016\n",
      "Removed URLs. New count of text with URLs: 0\n",
      "\n",
      "Text with usernames: 3950\n",
      "Removed usernames. New count of text with usernames: 0\n",
      "\n",
      "Text with numeric words: 1260\n",
      "Removed numeric words. New count of text with numeric words: 0\n",
      "\n",
      "Text with HTML tags: 0\n",
      "Removed HTML tags. New count of text with HTML tags: 0\n",
      "\n",
      "Text with newlines: 0\n",
      "Removed newlines. New count of text with newlines: 0\n",
      "\n",
      "Converted all texts to lowercase.\n",
      "\n",
      "Text with 'RT': 1031\n",
      "Removed 'RT'. New count of text with 'RT': 0\n",
      "\n",
      "Identified 3016 contractions. Attempting to expand some of them.\n",
      "Expanded contractions. Remaining contractions: 1026 (Some are possessive.)\n",
      "\n",
      "Text with possessives: 856\n",
      "Removed possessives. New count of text with possessives: 0\n",
      "\n",
      "Text with punctuations: 16770\n",
      "Removed punctuations. New count of text with punctuations: 0\n",
      "\n",
      "Removed stopwords from all texts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_X = cleaning_pipeline(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18464\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_X))\n",
    "# print(len(cleaned_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['breaking vcm inside novotel cubao owned mar roxas',\n",
       " 'onlybinaymagaangatsatacloban',\n",
       " 'political ad respect response duterte rape remark poe clarifies',\n",
       " 'wow binay bayan haha',\n",
       " 'roxas2016 roxasrobredo switchtomar',\n",
       " 'bets ready wrong bash alma moreno interview vp binay',\n",
       " 'pht',\n",
       " 'nakakairita commercial binay',\n",
       " 'binay binay binay',\n",
       " 'lead uprising binay wins trillanes']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_unique_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to cleaned_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "# dataset.save_to_file(cleaned_X, cleaned_Y, CLEANED_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to cleaned_dataset_w_dupes.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_file(cleaned_X, Y, 'cleaned_dataset_w_dupes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Dataset from Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New train-val-test split after data cleaning: \n",
    "- Train: 8674\n",
    "- Validation: 3859\n",
    "- Test: 3976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from cleaned_dataset_w_dupes.pkl\n"
     ]
    }
   ],
   "source": [
    "cleaned_dataset = Dataset(full_data_path= 'cleaned_dataset_w_dupes.pkl',\n",
    "                  from_scratch=False,\n",
    "                  split_sizes = split_sizes)\n",
    "cleaned_dataset.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inaasahan vice president jejomar binay taong', 'mar roxas tang ina tuwid daan daw eh sya nga straight', 'salamat sawang suporta taga makati pagbabalik binay makati onlybinayinmakatisankapa', 'putangina mo binay takbo', 'binay selective amnesia forgetting past six years spent preparing president pilipinasdebates2016']\n",
      "[0, 1, 0, 1, 0]\n",
      "18464\n",
      "18464\n"
     ]
    }
   ],
   "source": [
    "X = cleaned_dataset.get_features()\n",
    "Y = cleaned_dataset.get_labels()\n",
    "\n",
    "print(X[:5])\n",
    "print(Y[:5])\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inaasahan vice president jejomar binay taong', 'mar roxas tang ina tuwid daan daw eh sya nga straight', 'salamat sawang suporta taga makati pagbabalik binay makati onlybinayinmakatisankapa', 'putangina mo binay takbo', 'binay selective amnesia forgetting past six years spent preparing president pilipinasdebates2016']\n",
      "[0, 1, 0, 1, 0]\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_train = cleaned_dataset.get_features(split_type=\"train\")\n",
    "Y_train = cleaned_dataset.get_labels(split_type=\"train\")\n",
    "\n",
    "print(X_train[:5])\n",
    "print(Y_train[:5])\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['escudero denies betraying poe meeting binay karekare sinampalukan topic balimbing', 'hndi one chance saka binay fb haist', 'mar roxas addressing crowd gathered pasay city ulat barangay', 'perfect makaharap duterte binay makatikim mura', 'onlybinaypriority4ps nating hayaan maloko pulitikong binay']\n",
      "[0, 1, 0, 0, 0]\n",
      "4232\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "X_val = cleaned_dataset.get_features(split_type=\"val\")\n",
    "Y_val = cleaned_dataset.get_labels(split_type=\"val\")\n",
    "\n",
    "print(X_val[:5])\n",
    "print(Y_val[:5])\n",
    "print(len(X_val))\n",
    "print(len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unshaded votes votes mayor duterte goes mar roxas according reports ballot tests ayawsadilaw', 'nomorechance', 'well good choices like duterte poe still undecided think', 'nognog pandak laki hirap corrupt binay', 'exbinay aide turns tables mercado']\n",
      "[1, 1, 0, 1, 0]\n",
      "4232\n",
      "4232\n"
     ]
    }
   ],
   "source": [
    "X_test = cleaned_dataset.get_features(split_type=\"test\")\n",
    "Y_test = cleaned_dataset.get_labels(split_type=\"test\")\n",
    "\n",
    "print(X_test[:5])\n",
    "print(Y_test[:5])\n",
    "print(len(X_test))\n",
    "print(len(Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
